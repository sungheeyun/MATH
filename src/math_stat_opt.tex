\documentclass[10pt, twoside]{book}   	% use "amsart" instead of "article" for AMSLaTeX format

\newcommand{\theoremsandsuchchapter}{}

\input{mydefs}

\usepackage{appendix}
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ...
%\geometry{landscape}                		% Activate for rotated page geometry
\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex
\usepackage{amssymb}
\usepackage{mathtools}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    %urlcolor=cyan,
    urlcolor=blue,
}

\usepackage{amsmath}

%SetFonts

%SetFonts

\newcommand{\feasibleset}{\mathcal{F}}
\newcommand{\optsolset}{\mathcal{X}^\ast}
\newcommand{\grad}{\nabla}
\newcommand{\possemidefset}[1]{\mathcal{S}_+^{#1}}
\newcommand{\posdefset}[1]{\mathcal{S}_{++}^{#1}}
\newcommand{\covmat}[1]{{\Sigma}_{#1}}


% statistics commands

%\newcommand{\prob}[1]{\Prob\{#1\}}
%\newcommand{\condprob}[2]{\Prob\set{#1}{#2}}
\newcommand{\prob}[1]{P(#1)}
\newcommand{\condprob}[2]{P(#1|#2)}


\title{Mathematics, Statistics, Optimization, and Machine Learning}
\author{Sunghee Yun}
%\date{}							% Activate to display a given date or no date


%\includeonly{app-amazon-mlu}
%\includeonly{part-statistics}
%\includeonly{chp-rl}

\begin{document}

\maketitle

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{3}
\tableofcontents

%\newpage

\part{Mathematics}

\chapter{Calculus}

\section{Basics}

\begin{theorem}
[L'H\^opital's rule]
Let $f:\reals\to\reals$ and $f:\reals\to\reals$ be differentiable on an open interval $I\subseteq \reals$
except possibly at $c\in I$.
If $\lim_{x\to c} f(x) = \lim_{x\to c} g(x) = 0$ or $\pm \infty$,
$g'(x)=0$ for all $x\in I\backslash \{c\}$,
and $\lim_{x\to c} \frac{f'(x)}{g'(x)}$ exists,
then
\begin{equation}
\label{eq:lhopital-rule}
\lim_{x\to c} \frac{f(x)}{g(x)}
= \lim_{x\to c} \frac{f'(x)}{g'(x)}.
\end{equation}
\end{theorem}

\begin{definition}
[Taylor polynomial]
Let $n\in\integers$ be a positive integer and
let $f:\reals\to\reals$ be $n$ times differentiable at $a\in\reals$.
The $n$-th order Taylor polynomial is defined by
\begin{eqnarray}
T_{f,n}(x) &=& f(a) + f'(a)(x-a)
+ \frac{f''(a)}{2!}(x-a)^2
+ \cdots
+ \frac{f^{(n)}(a)}{n!}(x-a)^n
\nonumber
\\
&=&
\sum_{k=0}^n \frac{f^{(k)}(a)}{k!} (x-a)^k
\label{eq:taylor-poly}
\end{eqnarray}
\end{definition}


\begin{theorem}
[Taylor's theorem]
\label{theorem:taylor-peano}
Let $n\in\integers$ be a positive integer and
let $f:\reals\to\reals$ be $n$ times differentiable at $a\in\reals$.
Then there exists a function $h_n:\reals\to\reals$ such that
\begin{equation}
\label{eq:taylor-peano}
f(x) = T_{f,n}(x) + h_n(x) (x-a)^n
\end{equation}
and $\lim_{x\to a} h_n(x)=0$.
The remainder is called the Peano form of the remainder.
\end{theorem}

\begin{theorem}
[Taylor's theorem]
\label{theorem:taylor-lagrange}
Let $n\in\integers$ be a positive integer, $a,b\in\reals$,
and $I_o = (a,b) \cup (b,a)$ and $I_c = [a,b] \cup [b,a]$.
Let $f:\reals\to\reals$ be $n+1$ times differentiable on $I_o$
and $f^{(n)}$ is continuous on $I_c$.
Then for some $c \in I_o$,
\begin{equation}
\label{eq:taylor-lagrange}
f(b) = T_{f,n}(b) + \frac{f^{(n+1)}(c)}{(n+1)!}(b-a)^{n+1}.
\end{equation}
The remainder is called the Peano form of the remainder.
\end{theorem}


\section{Multivariate functions}

\begin{definition}[Jacobian matrix]
Let $f:\reals^n \to \reals^m$ be a differentiable function,
\ie, the partial derivative $\partial f_j(x) / \partial x_i$ exists for every $1\leq i\leq n$ and $1\leq j\leq m$.
Then the Jacobian matrix of $f$ at $x$ is defined by the function $D_f:\reals^{n} \to \reals^{m \times n}$
such that
\begin{equation}
\label{eq:jacobian-matrix}
D_f(x) = \begin{my-matrix}{cccc}
\partial f_1(x) / \partial x_1
& \partial f_1(x) / \partial x_2
& \cdots
& \partial f_1(x) / \partial x_n
\\
\partial f_2(x) / \partial x_1
& \partial f_2(x) / \partial x_2
& \cdots
& \partial f_2(x) / \partial x_n
\\
\vdots & \vdots & \ddots & \vdots
\\
\partial f_m(x) / \partial x_1
& \partial f_m(x) / \partial x_2
& \cdots
& \partial f_m(x) / \partial x_n
\end{my-matrix}
\in\reals^{m \times n}.
\end{equation}
\end{definition}



\section{Chain rule}

\begin{theorem}
\label{theorem:chain-rule}
Let $f:\reals\to\reals^n$ and $g:\reals^n\to\reals$ be differentiable.
Then $h:\reals\to\reals$ such that $h(t) = g(f(t))$ is also differentiable and
\[
h'(t) = \sum_{i=1}^n f_i'(t) \frac{\partial g}{\partial x_i} (f(t))
= \nabla^T g(f(t))^T D_f (t)
\]
for all $t\in\dom f$.
\end{theorem}

\begin{corollary}
\label{corollary:chain-rule-gen}
Let $f:\reals^n\to\reals^m$ and $g:\reals^m\to\reals^p$ be differentiable.
Then define a function $h:\reals^n\to\reals^p$ such that $h(x) = g(f(x))$ for all $x\in\dom f$.
Then $h$ is differentiable and
\begin{equation}
\label{eq:chain-rule-gen}
D h(x) = Dg(f(x)) Df(x)
\end{equation}
where
$D f:\reals^n\to\reals^{m\times n}$,
$D g:\reals^m\to\reals^{p\times m}$,
and $D f:\reals^n\to\reals^{p\times n}$
are the Jacobian matrix functions of $f$, $g$, and $h$ respectively.
\end{corollary}

\begin{corollary}
\label{corollary:dixu}
Let $f:\reals^n\to\reals$ be differentiable.
Then for some $A\in\reals^{n\times m}$ and $b\in\reals^n$,
define $g:\reals^m\to\reals$ such that $g(y) = f(Ay+b)$.
Then
\begin{equation}
\label{eq:dixu}
\nabla g(y) = A^T \nabla f(Ay+b).
\end{equation}
\end{corollary}


\begin{corollary}
\label{corollary:eicg}
Let $f:\reals^n\to\reals$ be twice differentiable.
Then for some $A\in\reals^{n\times m}$ and $b\in\reals^n$,
define $g:\reals^m\to\reals$ such that $g(y) = f(Ay+b)$.
Then
\begin{equation}
\label{eq:eicg}
\nabla^2 g(y) = A^T \nabla^2 f(Ay+b)A.
\end{equation}
\end{corollary}

\section{Integration}


\begin{lemma}
Let $A\in\reals^{n\times n}$ be a nonsingular matrix. Suppose that the following integral exists for some $C\subseteq \reals^n$.
\begin{equation}
\int_{C} f(x) dx
\end{equation}
\end{lemma}





\chapter{Convex analysis}

\section{Convex function}

A function $f:\reals^n\to\reals$ is a convex function if, for all $x,y \in \dom f$ and all $0\leq \lambda \leq 1$,
\begin{equation}
f(\lambda x + (1-\lambda)y)
\leq
\lambda f(x) + (1-\lambda)f(y).
\end{equation}

\begin{theorem}
\label{theorem:cvx-equiv-1d-fcn}
Let $f:\reals^n\to\reals$.
Then for some $x\in\dom f$ and $v\in\reals^n$,
define a function $g_{x,v}(t):\reals\to\reals$ such that $g_{x,v}(t) = f(x+tv)$
with the domain, $\set{t\in\reals}{x+tv\in \dom f}$.
Then $f$ is a convex function iff $g_{x,v}$ is a convex function for any $x\in\dom f$ and any $v\in\reals^n$.
\end{theorem}

\begin{proof}
Suppose that $f$ is a convex function.
Then for any $x\in\dom f$ and $v\in\reals^n$,
for any $s,t \in\set{t\in\reals}{x+tv\in \dom f}$ and any $\lambda\in\reals$ such that $0\leq \lambda \leq 1$,
\begin{eqnarray*}
\lefteqn{
g_{x,v}(\lambda s + (1-\lambda) t)
=f(x+(\lambda s + (1-\lambda) t)v)
}
\\
&=&
f(\lambda (x+sv) + (1-\lambda) (x+tv))
\\
&\leq&
\lambda f(x+sv) + (1-\lambda) f(x+tv)
= \lambda g_{x,v}(s) + (1-\lambda) g_{x,v}(t).
\end{eqnarray*}
Therefore $g_{x,v}$ is a convex function.

Now assume that
$g_{x,v}(t):\reals\to\reals$ is a convex function for any $x\in\dom f$ and $v\in\reals^n$.
Then for any $x,y \in \dom f$ and any $\lambda\in\reals$ such that $0\leq \lambda \leq 1$,
\begin{eqnarray*}
\lefteqn{
f((1-\lambda) x + \lambda y ) =f(x + \lambda (y-x) )
}
\\
&=&
g_{x,y-x}(\lambda)
= g_{x,y-x}((1-\lambda)\cdot 0 + \lambda \cdot 1)
\leq (1-\lambda) g_{x,y-x}(0) + \lambda g_{x,y-x}(1)
\\
&=&
(1-\lambda) f(x) + \lambda f(y),
\end{eqnarray*}
thus, $f$ is a convex function.
\end{proof}

\subsection{First order condition}

\begin{theorem}
\label{theorem:cvx-1st-order-cond-1}
If a function $f:\reals\to\reals$ is differentiable, it is a convex function iff, for all $x, y \in \dom f$,
\begin{equation}
\label{eq:cvx-1st-order-cond-1}
        f(y) \geq f(x) + f'(x) (y-x).
\end{equation}
\end{theorem}

\begin{proof}
Suppose that $f$ is a convex function.
Then assume that $y>x$. Let $h\in\reals$ be a positive number such that $h<y-x$. Then the definition of convexity implies that
\[
f(x+h) \leq (1-\lambda) f(x) + \lambda f(y)
\]
where $\lambda = h/(y-x)$ since
\[
(1-\lambda) x + \lambda y = x + \lambda (y-x) = x +h.
\]
Thus
\[
f(x+h) - f (x) \leq \lambda (f(y)-f(x)) = \frac{h}{y-x} (f(y)-f(x)),
\]
which implies
\[
f'(x) = \lim_{h\to0} \frac{f(x+h) - f (x)}{h} \leq \frac{f(y)-f(x)}{y-x}.
\]
Therefore
\[
f(y) - f(x) \geq f'(x)(y-x),
\]
hence (\ref{eq:cvx-1st-order-cond-1}) is true when $y>x$.

We can prove (\ref{eq:cvx-1st-order-cond-1}) is true when $y<x$ using the very same method.
Assume that $x>y$. Let $h\in\reals$ be a positive number such that $h<x-y$. Then the definition of convexity implies that
\[
f(x-h) \leq (1-\lambda) f(x) + \lambda f(y)
\]
where $\lambda = h/(x-y)$ since
\[
(1-\lambda) x + \lambda y = x + \lambda (y-x) = x -h.
\]
Thus
\[
f(x) - f (x-h) \geq \lambda (f(x)-f(y)) = \frac{h}{x-y} (f(x)-f(y)),
\]
which implies
\[
f'(x) = \lim_{h\to0} \frac{f(x) - f (x-h)}{h} \geq \frac{f(x)-f(y)}{x-y}  =\frac{f(y)-f(x)}{y-x}.
\]
Therefore
\[
f(y) - f(x) \geq f'(x)(y-x),
\]
hence (\ref{eq:cvx-1st-order-cond-1}) is true when $y<x$.
It is obvise that (\ref{eq:cvx-1st-order-cond-1}) is true when $y=x$.
Hence we have just proved that if $f:\reals\to\reals$ is a convex function, then (\ref{eq:cvx-1st-order-cond-1}) holds
for any $x,y\in\dom f$.

Now we prove the converse.
Suppose that (\ref{eq:cvx-1st-order-cond-1}) holds for any $x,y\in\dom f$.
Now let $x,y\in\dom f$ and $\lambda \in \reals$ suc that $0\leq \lambda \leq 1$.
Let $z=\lambda x + (1-\lambda) y$. Then (\ref{eq:cvx-1st-order-cond-1}) implies that
\begin{equation}
\label{eq:dkfj-1}
f(x) - f(z) \geq f'(z) (x-z) = (1-\lambda) f'(z) (x-y)
\end{equation}
and
\begin{equation}
\label{eq:dkfj-2}
f(y) - f(z) \geq f'(z) (y-z) = \lambda f'(z) (y-x)
\end{equation}
If we multiply $\lambda$ on both sides of (\ref{eq:dkfj-1}),
multiply $1-\lambda$ on both sides of (\ref{eq:dkfj-2}),
and add both sides, we have
\[
\lambda(f(x) - f(z)) + (1-\lambda) (f(y) - f(z))
\geq \lambda f(x) +(1-\lambda) f(y) - f(z) \geq 0,
\]
hence
\[
f(\lambda x + (1-\lambda) y)
\leq \lambda f(x) + (1-\lambda) f(y).
\]
Therefore $f$ is a convex function.
\end{proof}

\begin{corollary}
\label{corollary:cvx-deriv-non-decreasing}
Let $f:\reals\to\reals$ be differentiable. Then $f$ is a convex function iff the derivative of $f$ is a nondecreasing function.
\end{corollary}
\begin{proof}
Suppose that $f$ is a convex function.
Let $x,y\in\dom f$ such that $x<y$.
Then \theoremname~\ref{theorem:cvx-1st-order-cond-1} implies
\[
f(y) \geq f(x) + f'(x)(y-x)
\]
and
\[
f(x) \geq f(y) + f'(y)(x-y),
\]
thus
\[
f'(x) \leq \frac{f(y)-f(x)}{y-x}
= \frac{f(x)-f(y)}{x-y} \leq f'(y)
\]
since $y>x$.
Therefore $f'$ is a nondecreasing function.

Now we prove the converse. Suppose that $f'$ is a nondecreasing function.
Then if $y>x$, the mean value theorem implies that there exists some $z \in (x,y)$ such that
\[
\frac{f(y)-f(x)}{y-x} = f'(z).
\]
Since $f'$ is nondecreasing, we have
\[
f'(x) \leq \frac{f(y)-f(x)}{y-x} \leq f'(y),
\]
thus
\begin{equation}
\label{eq:cias-1}
f(y) \geq f(x) + f'(x)(y-x)
\end{equation}
and
\begin{equation}
\label{eq:cias-2}
f(y) \leq f(x) + f'(y)(y-x).
\end{equation}
Therefore (\ref{eq:cias-1}) implies that $f$ satisfies (\ref{eq:cvx-1st-order-cond-1}).
Now if $x>y$, (\ref{eq:cias-2}) implies that
\[
f(x) \leq f(y) + f'(x)(x-y)
\Leftrightarrow
f(y) \geq f(x) + f'(x)(y-x)
\]
which again implies that $f$ satisfies (\ref{eq:cvx-1st-order-cond-1}).
Therefore (\ref{theorem:cvx-1st-order-cond-1}) implies that $f$ is a convex function.
\end{proof}

\begin{corollary}
\label{corollary:cvx-1st-order-cond}
If a function $f:\reals^n\to\reals$ is differentiable, it is a convex function iff, for all $x,y \in \dom f$,
\begin{equation}
\label{eq:cvx-1st-order-cond}
        f(y) \geq f(x) + \nabla f(x)^T (y-x).
\end{equation}
\end{corollary}

\begin{proof}
Suppose that $f$ is a convex function.
Let $x,y\in\dom f$.
If we let $g_{x,y-x}:\reals\to\reals$ be a function such that $g_{x,y-x}(t) = f(x+t(y-x))$,
\theoremname~\ref{theorem:cvx-equiv-1d-fcn} implies $g_{x,y-x}$ is a convex function.
Therefore \theoremname~\ref{theorem:cvx-1st-order-cond-1} together with \corollaryname~\ref{corollary:dixu}
implies
\[
f(y) = g_{x,y-x}(1) \geq g_{x,y-x}(0) + g_{x,y-x}'(0) (1-0)
= f(x) + \nabla f(x) ^T (y-x)
\]
for any $x,y \in \dom f$.

Now suppose that (\ref{eq:cvx-1st-order-cond}) holds for any $x,y \in \dom f$.
Then \corollaryname~\ref{corollary:dixu} implies that,
for any $r,s\in\reals$ and $v\in\reals^n$ such that $r,s\in\set{t\in\reals}{x+tv\in\dom f}$,
\[
g_{x,v}(r) = f(x+rv) \geq f(x+sv) + (r-s) \nabla f(x+sv)^T v = g_{x,v}(s) + g_{x,v}'(s)(r-s).
\]
Thus \theoremname~\ref{theorem:cvx-1st-order-cond-1} implies
$g_{x,v}$ is a convex function for any $x\in\dom f$ and $v\in\reals^n$.
Therefore by \theoremname~\ref{theorem:cvx-equiv-1d-fcn}, $f$ is a convex function.
\end{proof}

\subsection{Second order condition}

\begin{theorem}
\label{theorem:cvx-2nd-order-cond-1}
If a function $f:\reals\to\reals$ is twice differentiable, it is a convex function iff, for all $x \in \dom f$,
\begin{equation}
\label{eq:cvx-2nd-order-cond-1}
        f''(x) \geq 0.
\end{equation}
\end{theorem}

\begin{proof}
Suppose that $f$ is a convex function.
Then \corollaryname~\ref{corollary:cvx-deriv-non-decreasing} implies that $f'$ is a nondecreasing function,
hence
\[
f''(x)
= \lim_{h\to0} \frac{f'(x+h) - f'(x)}{h}
= \lim_{h\to0^+} \frac{f'(x+h) - f'(x)}{h} \geq 0.
\]
Now if $f''(x)\geq0$ for all $x\in\dom f$, the mean value theorem implies that $f'$ is a nondecreasing function.
\end{proof}


\begin{theorem}
\label{theorem:cvx-2nd-order-cond}
If a function $f:\reals^n\to\reals$ is twice differentiable, it is a convex function iff, for all $x \in \dom f$,
\begin{equation}
\label{eq:cvx-2nd-order-cond}
        \nabla^2 f(x) \succeq 0.
\end{equation}
\end{theorem}

\begin{proof}
Suppose that $f$ is a convex function.
Then \theoremname~\ref{theorem:cvx-equiv-1d-fcn} implies that
for any $x\in\dom f$ and any $v\in\reals^n$,
the function $g_{x,v}:\reals\to\reals$ such that $g_{x,v}(t) = f(x+tv)$
is a convex function in $\set{t\in\reals}{x+tv \in \dom f}$.
Then \theoremname~\ref{theorem:cvx-2nd-order-cond-1} together with \corollaryname~\ref{corollary:eicg}
implies that
\[
v^T \nabla^2 f(x) v = g_{x,v}''(0) \geq0.
\]
Therefore $\nabla^2 f(x) \succeq 0$ for any $x \in \dom f$.

Now if $\nabla^2 f(x) \succeq 0$ for all $x\in\dom f$,
then
\corollaryname~\ref{corollary:eicg}
implies that
$g_{x,v}''(t) = v^T \nabla^2 f(x+tv) v \geq0$ for any $x \in \dom f$ and any $v\in\reals^n$.
Then \theoremname~\ref{theorem:cvx-2nd-order-cond-1} implies
$g_{x,v}$ is a convex function for any $x \in \dom f$ and any $v\in\reals^n$,
hence by \theoremname~\ref{theorem:cvx-equiv-1d-fcn},
$f$ is a convex function.
\end{proof}



\chapter{Linear Algebra}

\section{Vector space}

Cauchy–Schwarz inequality: for $a,b\in\complexes^n$,
\begin{equation}
|a^H b| \leq \|a\|_2 \|b\|_2.
\end{equation}

The generalized form:
for $f:[0,1] \to \complexes$ and $g:[0,1] \to \complexes$,
\begin{equation}
\left|\int_{0}^1 \overline{f(t)} g(t) \, dt \right|
\leq
\left(\int_0^1 |f(t)|^2 dt \, \right)^{1/2}
\left(\int_0^1 |g(t)|^2 dt \, \right)^{1/2}.
\end{equation}

H\"older's inequality: for $a,b\in\complexes^n$, $p>1$, and $q>1$ such that $1/p+1/q=1$,
\begin{equation}
|a^H b| \leq \|a\|_p \|b\|_q.
\end{equation}

The generalized form:
for $f:[0,1] \to \complexes$, $g:[0,1] \to \complexes$,
$p>1$, and $q>1$ such that $1/p+1/q=1$,
\begin{equation}
\left|\int_{0}^1 \overline{f(t)} g(t) \, dt \right|
\leq
\left(\int_0^1 |f(t)|^p dt \, \right)^{1/p}
\left(\int_0^1 |g(t)|^q dt \, \right)^{1/q}.
\end{equation}

When $b=\ones$, the Cauchy-Schwarz inequality implies
\begin{equation}
\|a\|_1 \leq n \|a\|_2
\end{equation}



\section{Eigenvalues}

\subsection{Basic definitions}

Given a square matrix $A\in\reals^{n\times n}$,
if there exist $\lambda \in \complexes$ and nonzero $v \in \complexes^n$ such that
\begin{equation}
        A v = \lambda v
\end{equation}
then $\lambda$ is called an eigenvalue of $A$ and $v$ is called an eigenvector associated with $\lambda$.

If there exist $n$ linearly independent eigenvectors, we have
\begin{equation}
A \begin{my-matrix}{ccc} v_1 & \cdots & v_n \end{my-matrix}
= \begin{my-matrix}{ccc} v_1 & \cdots & v_n \end{my-matrix} \diag(\lambda_1,\ldots,\lambda_n)
\end{equation}
or
\begin{equation}
\label{eq:v8dy}
A V = V \Lambda
\end{equation}
where
\begin{equation}
V = \begin{my-matrix}{ccc} v_1 & \cdots & v_n \end{my-matrix}
\in\complexes^{n\times n}
\end{equation}
and
\begin{equation}
\Lambda = \diag(\lambda_1,\ldots,\lambda_n)
= \begin{my-matrix}{cccc}
\lambda_1 & 0 & \cdots & 0
\\
0 & \lambda_2 & \cdots & 0
\\
\vdots & \vdots & \ddots & \vdots
\\
0 & 0 & \cdots & \lambda_n
\end{my-matrix}
\in\complexes^{n\times n}.
\end{equation}
In this case, $A$ is said to be diagonalizable.

Since $V$ is nonsingular, \ie, invertible, we can rewrite (\ref{eq:v8dy}) as
\begin{equation}
\label{eq:2}
A = V \Lambda V^{-1} \Leftrightarrow V^{-1} A V = \Lambda.
\end{equation}


\subsection{Symmetric matrices}

Given a symmetric matrix $A = A^T\in\reals^{n\times n}$,
all the eigenvalues are real and we can choose $n$ real orthonormal eigenvectors,
\ie,
we can find $n$ eigenvectors $v_1, \ldots, v_n\in\reals^n$
associated with $n$ eigenvectors, $\lambda_1, \ldots, \lambda_n \in \reals$
such that
\begin{equation}
    \|v_i\| = 1
\end{equation}
for $i=1,\ldots,n$
and
\begin{equation}
    v_i^T v_j = 0
\end{equation}
for $1\leq i\neq j\leq n$.
Thus, all symmetric matrices are diagonalizable.

Now (\ref{eq:2}) becomes
\begin{equation}
\label{eq:sym-eigen-decomp}
A = V \Lambda V^T \Leftrightarrow V^T A V = \Lambda
\end{equation}
since
\begin{equation}
V^T V = I_n
\end{equation}
where $I_n\in\reals^{n\times n}$ is the indentity matrix.
We can rewrite (\ref{eq:sym-eigen-decomp}) as
\begin{equation}
\label{eq:sym-eigen-decomp-1}
A =
\begin{my-matrix}{ccc} v_1 & \cdots & v_n \end{my-matrix}
\diag(\lambda_1,\ldots,\lambda_n)
\begin{my-matrix}{c} v_1^T \\ \vdots \\ v_n^T \end{my-matrix}
= \sum_{i=1}^n \lambda_i v_i v_i^T.
\end{equation}

\section{Positive definiteness}

\begin{itemize}

\item
A symmetric matrix $A=A^T\in\reals^{n\times n}$ is called positive semidefinite if for all $x\in\reals^n$,
\begin{equation}
x^T A x \geq 0.
\end{equation}

\item
A symmetric matrix $A=A^T\in\reals^{n\times n}$ is called positive definite if for all nonzero $x\in\reals^n$,
\begin{equation}
x^T A x > 0.
\end{equation}

\item The set of all the $n$-by-$n$ positive semidefinite matrices is (sometimes) denoted by $\possemidefset{n}$,
\ie,
\begin{equation}
\possemidefset{n} = \set{A=A^T\in\reals^{n\times n}}{x^T A x \geq 0 \mbox{ for all } x \in \reals^n}.
\end{equation}

\item The set of all the $n$-by-$n$ positive definite matrices is (sometimes) denoted by $\posdefset{n}$,
\ie,
\begin{equation}
\posdefset{n} = \set{A=A^T\in\reals^{n\times n}}{x^T A x > 0 \mbox{ for all nonzero } x \in \reals^n}.
\end{equation}

\item $A=A^T\in\reals^{n\times n}$ is positive semidefinite if and only if all the eigenvalues of $A$ are nonnegative.

\item $A=A^T\in\reals^{n\times n}$ is positive definite if and only if all the eigenvalues of $A$ are positive.

\begin{proof}
For symmetric $A=A^T$, there exist orthgonal $V\in\reals^{n\times n}$ and diagonal $\Lambda\in\reals^{n\times n}$
such that
\[
A = V \Lambda V^T = \sum_{i=1}^n \lambda_i v_i v_i^T,
\]
thus for any $x\in\reals^n$,
\[
x^T A x = x^T \left(\sum_{i=1}^n \lambda_i v_i v_i^T \right) x
= \sum_{i=1}^n \lambda_i x^T v_i v_i^T x
= \sum_{i=1}^n \lambda_i (v_i^T x)^2.
\]
Therefore if all $\lambda_i$ are nonnegative, $x^T A x\geq0$ for any $x\in\reals^n$, hence $A\in\possemidefset{n}$.
Now assume $A\in\possemidefset{n}$, but $\lambda_j < 0$ for some $j\in\{1,\ldots,n\}$.
Then
\begin{equation}
v_j^T A v_j
= \sum_{i=1}^n \lambda_i (v_i^T v_j)^2
= \sum_{i=1}^n \lambda_i \delta_{i,j}
= \lambda_j < 0
\end{equation}
since $v_1$, \ldots, $v_n$ are orthonormal
where
$\delta_{i,j}$ is the \href{https://en.wikipedia.org/wiki/Kronecker_delta}{Kronecker delta function},
hence $A\not \in \possemidefset{n}$.
Therefore if $A\in\possemidefset{n}$, all $\lambda_i$ are nonnegative.

Therefore $A\in\possemidefset{n}$ if and only if all $\lambda_i$ are nonnegative.

Now assume that all $\label_i$ are positive.
Then for all nonzero $x\in\reals^n$,
there exists $i\in\{1,\ldots,n\}$ such tat $v_i^Tx$
since if $v_i^Tx=0$ for all $i$, then
$V^T x = 0$, hence $x=0$ since $V^T$ is nonnsigular.
Therefore
\begin{equation}
x^T A x = \sum_{i=1}^n \lambda_i (v_i^T x)^2
\geq \lambda_j (v_j^T x)^2 > 0.
\end{equation}
Thus, $A\in\posdefset{n}$.

Now assume that $A\in\posdefset{n}$.
If $\lambda_j \leq 0$ for some $j\in\{1,\ldots,n\}$,
then
\begin{equation}
v_j^T A v_j
= \sum_{i=1}^n \lambda_i \delta_{i,j}
= \lambda_j \leq 0,
\end{equation}
hence $A\not\in\posdefset{n}$. Therefore if $A\in\posdefset{n}$, all $\lambda_i$ are positive.

Therefore $A\in\posdefset{n}$ if and only if all $\lambda_i$ are positive.

\end{proof}

\end{itemize}

\section{Matrix norms}

\begin{equation}
\mathbf{dist}( C_\mathrm{org 1}, C_\mathrm{org 2} ) = 
\|C_\mathrm{org 1} - C_\mathrm{org 2} \|
= |\lambda_\mathrm{max}(C_\mathrm{org 1} - C_\mathrm{org 2})|
\end{equation}



\part{Optimization}

\include{chp-opt}

\chapter{Portfolio optimization}

\section{Problem formulation}

Suppose that we have $n$ assets to invest on
and that the return of each asset per unit invest is modeled by random variables $R_i$ for $i=1,\ldots,n$.
Then we want to decide the amount of investment on each asset, $x_i\in\reals$ for $i=1,\ldots,n$,
so that it optimizes the overall investment (in certain senses).

For formulization, we use the following definitions.

\begin{itemize}
\item Define a vector random variable $R\in\reals^n$ such that
\begin{equation}
R = \begin{my-matrix}{c}
R_1
\\
\vdots
\\
R_n
\end{my-matrix}
\in\reals^n.
\end{equation}

\item Let $r\in\reals^n$ be the expected value of $R$,
\ie,
\begin{equation}
r
= \Expect(R)
= \begin{my-matrix}{c}
\Expect(R_1)
\\
\vdots
\\
\Expect(R_n)
\end{my-matrix}
= \begin{my-matrix}{c}
r_1
\\
\vdots
\\
r_n
\end{my-matrix}
\in\reals^n.
\end{equation}

\item Define a vector $x\in\reals$ which is an aggregate of the investments:
\begin{equation}
x = \begin{my-matrix}{c}
x_1
\\
\vdots
\\
x_n
\end{my-matrix}
\in\reals^n.
\end{equation}

\item Define a feasible set $\mathcal{X}\subseteq \reals^n$ for $x$.
For example, if we have a limit on the total investment,
\begin{equation}
\label{eq:cnst-on-cost}
\mathcal{X} = \set{x\in\reals^n}{\sum_{i=1}^n c_i x_i \leq c_\mathrm{max}},
\end{equation}
or if we have the minimum and maximum amount to invest for each asset,
we'd have
\begin{equation}
\label{eq:cnst-on-each-amount}
\mathcal{X} = \set{x\in\reals^n}{d_\mathrm{min} \leq x_i \leq d_\mathrm{max} \mbox{ for } i=1,\ldots,n}.
\end{equation}
Generally, we'd prefer $\mathcal{X}$ to be a convex set, \ie,
for any $x,y\in\mathcal{X}$ and $0\leq \lambda \leq 1$,
\begin{equation}
\lambda x + (1-\lambda) y \in \mathcal{X}.
\end{equation}

\end{itemize}

\subsection{A portfolio optimization problem}

A portfolio optimization problem can be formulized by
\begin{equation}
\label{eq:opt-port-prob}
\begin{array}{ll}
\mbox{maximize} & f(x) = \Expect(Z)
\\
\mbox{minimize} & g(x) = \Var(Z)
\\
\mbox{subject to} & x \in \mathcal{X}
\end{array}
\end{equation}
where the optimization variable is $x\in\reals^n$
and
\begin{equation}
Z = \sum_{i=1}^n x_i R_i = x^T R
\end{equation}
where $\Expect(\cdot)$ and $\Var(\cdot)$ refer to the expected value and the variance operators respectively.

This problem formulation tries to \emph{maximize the expected return}
while \emph{minimizing the variance or uncertainty or risk}, which generally makes sense.

(\ref{eq:cnst-on-cost})
(\ref{eq:cnst-on-each-amount})


Note that
\begin{equation}
\Expect(Z)
= \Expect(x^T R)
= \Expect \left(\sum_{i=1}^n x_i R_i \right)
= \sum_{i=1}^n x_i \Expect(R_i)
= \sum_{i=1}^n x_i r_i
= r^T x
\end{equation}
and
\begin{eqnarray*}
\lefteqn{
\Var(Z) = \Expect(Z-\Expect(Z))^2 = \Expect \left(x^TR-x^Tr \right)^2
}
\\
&=&
\Expect \left(x^T(R-r) \right)^2
= \Expect \left(x^T(R-r)(R-r)^T x \right)
\\
&=&
x^T \Expect(R-r)(R-r)^T x
=
x^T \covmat{R} x
\end{eqnarray*}
where $\covmat{R} = \Expect(R-r)(R-r)^T$ is the \href{https://en.wikipedia.org/wiki/Covariance_matrix}{covariance matrix} of $R$.
Note that $\covmat{R}\in\possemidefset{n}$
since for any $y\in\reals^n$,
\begin{equation}
y^T \covmat{R} y = y^T \Expect(R-r)(R-r)^T y = \Expect(x^T(R-r))^2 \geq 0.
\end{equation}



Thus, (\ref{eq:opt-port-prob}) can be rewritten as
\begin{equation}
\label{eq:opt-port-prob-vec}
\begin{array}{ll}
\mbox{maximize} & f(x) = r^T x
\\
\mbox{minimize} & g(x) = x^T \covmat{R} x
\\
\mbox{subject to} & x \in \mathcal{X}.
\end{array}
\end{equation}




\part{Statistics}
\include{part-statistics}

\part{Machine Learning}

\chapter{Optimization}

\section{Gradient method}


Suppose that $f:\reals^n\to\reals$. An unconstrained optimization problem is

\begin{equation}
\begin{array}{ll}
\mbox{minimize} & f(x)
\end{array}
\end{equation}

The gradient method is

\begin{equation}
x^{k+1} = x^{k} - \alpha^k \nabla f(x^k)
\end{equation}




\section{Stochastic gradient method}


Suppose that $f_i:\reals^n\to\reals$ for $i=1,\ldots,n$. An unconstrained optimization problem is

\begin{equation}
\begin{array}{ll}
\mbox{minimize} & \frac{1}{n} \sum_{i=1}^n f_i(x)
\end{array}
\end{equation}

The gradient method is

\begin{equation}
x^{k+1} = x^{k} - \alpha^k \sum_{}^k \nabla f(x^k)
\end{equation}








\chapter{Bayesian Network}


\begin{equation}
\Pr\{X_1,\ldots,X_n\}
= \prod_{i=1}^n \Pr\set{X_i}{X_1,\ldots,X_{i-1}}
= \prod_{i=1}^n \Pr\set{X_i}{\mathbf{parent}(X_i)}
\end{equation}


\chapter{Collaborative Filtering}

\input{chp_cf}

%\newcommand{\numcus}{\ensuremath{n_C}}
%\newcommand{\numitem}{\ensuremath{n_I}}
%\newcommand{\nan}{\ensuremath{\mathrm{NaN}}}
%
%
%\section{Item-based collaborative filtering}
%
%The purpose of the item-based collaborative filtering is to recommend items using the information obatined from similar items.
%Since the information from other items help provide better recommendation to customers,
%it is called an \emph{item-based collaborative filtering}.
%
%We formulize an item-based collaborative filtering as follows.
%We assume that there are \numcus\ customers and \numitem\ items.
%We further assume that we have partial information as to the taste of each customer for each item.
%In typical cases,
%we have customers can leave reviews with ratings for $n$ items where $n \ll \numitem$.
%When we have hundreds of thousands of items, this makes the data structure extremely sparse.
%This fact plays a critical role when we calculate similarities among items or customers,
%or learn the matrix factorization-based collaborative filtering modles,
%e.g., using gradient descent (GD) method or alternative least squares (ALS) method.\footnote{
%    These models will be discussed in later sections.
%}
%
%Now suppose that we have the rating matrix
%\begin{equation}
%R \in (\reals\cup\{\nan\})^{\numcus \times \numitem}
%\end{equation}
%where $R_{ij} \in (\reals\cup\{\nan\})$ denotes \emph{certain score} corresponding to $i$th customer and $j$th item
%and \nan\ denotes we do not know the values.
%Note that this {certain score} can refer to rating of movies, but can mean virtually anything related to customers'
%taste or activities. For example, it can mean the frequency of customers' clicks on certain menus
%where the items mean the menu items for this example.
%
%This matrix is sparse, but not in a traditional way. Generally, we say a matrix is sparse if the number of nonzero entries
%is much less than $\numcus \times \numitem$. We say $R$ is sparse because there are huge number of entries
%for which we \emph{do not know} the true values for them.
%In a general recommendation problem, the purpose is to guess or estimate the true values,
%\eg, what rating the customer would give if she did, or how frequently a customer would click on the menu item if she knew
%there exist such menu item.
%
%Now we want to evalute the similarity (or distance) among items.
%In many places in the recommendation systems and information retrieval literature,
%it is shown that applying some transformation or weighting on the values
%before calculating the similarity measure,
%\eg, \href{https://en.wikipedia.org/wiki/Cosine_similarity}{cosine similarity}
%or \href{https://en.wikipedia.org/wiki/Correlation_coefficient}{correlation coefficient}\footnote{
%    The correlation coefficient is sometimes called the Pearson correlation coefficient
%    after \href{https://en.wikipedia.org/wiki/Karl_Pearson}{Karl Pearson}, who was an English mathematician and biostatistician.
%}.
%There weighting methods and similarity measures will be discussed in later sections.
%
%\subsection{Item-based collaborative filtering for menu personalization for mobile shopping app}
%
%Here we consider a problem of using a collaborative filtering for efficient menu personalization problem for an mobile shoppiong app.
%
%
%
%
%\section{Collaborative filtering using matrix factorization}
%
%The modeling matrices are
%\begin{eqnarray*}
%&A \in \preals^{M\times N}&
%\\
%&D \in \preals^{M\times K}&
%\\
%&M \in \preals^{K\times N}&
%\end{eqnarray*}
%where the number of machine learning (ML) parameters is $K(M+N)$.
%
%For example, if
%the number of users ($=M$) is $1$MM,
%the number of menus ($=N$) is 28,
%and
%the number of features ($=K$) we want to model is 10,
%the total number of ML parameters is $\sim 10$MM.

\chapter{Time Series Anomaly Detection}

\section{Real-Time Anomaly Detection}

\subsection{Computing Anomaly Likelihood}



At every time step, we evaluate the moving sample means and the moving sample standard deviations:
\begin{eqnarray}
\mu(t) &=& \frac{1}{w} \sum_{i=0}^{w-1} s(t-i)
\\
\sigma(t) &=& \sqrt{\frac{1}{w-1} \sum_{i=0}^{w-1} (s(t-i)-\mu(t))^2}
\end{eqnarray}
Then compute a recent short term average of raw anomaly scores,
and apply a threshold to the Gaussian tail probability (Q-function)
to decided whether or not to declare an anomaly:
\begin{equation}
L(t) = 1 - Q\left(\frac{\tilde{\mu}(t) - \mu(t)}{\sigma(t)}\right)
\end{equation}
where the short-term moving sample mean is defined by
\begin{equation}
\tilde{\mu}(t) = \frac{1}{w'} \sum_{0}^{w'-1} s(t-i)
\end{equation}
and the Q-function is defined by
\begin{equation}
Q(x) = \frac{1}{\sqrt{2\pi}} \int_x^\infty \exp\left(-\frac{\tau^2}{2}\right)  d \tau.
\end{equation}

We threshold $L(t)$ and report an anomaly if it is very close to $1$:
\begin{equation}
L(t) > 1 - \epsilon.
\end{equation}



To take longer history data into considering while simutaneously emphasizing recent data more,
we can consider the exponentially weighted moving sample mean and standard deviation:

\begin{eqnarray}
\mu(t) &=& (1-\gamma) \sum_{i=0}^{\infty} \gamma^i s(t-i)
\\
\sigma(t) &=& \sqrt{(1-\gamma) \sum_{i=0}^{\infty} \gamma^i (s(t-i)-\mu(t))^2}
\end{eqnarray}

To combine the advantages of finite window method and exponentially weighted method,
we can consider the exponentially weighted moving sample mean and standard deviation with finite window
as follows:
\begin{eqnarray}
\mu(t) &=& \frac{1}{\sum_{i=0}^{w-1} \gamma^i }\sum_{i=0}^{w-1} \gamma^i s(t-i)
\\
\sigma(t) &=& \sqrt{\frac{1}{\sum_{i=0}^{w-1} \gamma^i }\sum_{i=0}^{w-1} \gamma^i (s(t-i)-\mu(t))^2}
\end{eqnarray}


\chapter{Reinforcement Learning}
\include{chp-rl}


\iffalse
\appendix
\appendixpage
\addappheadtotoc


\chapter{Amazon Machine Learning University}

\include{app-amazon-mlu}

\fi

\end{document}
