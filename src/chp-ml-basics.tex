\newcommand{\dataset}{\ensuremath{{\mathcal{D}}}}

\section{Optimal Predictor}
\label{sec:opt-predictor}

Consider a regression problem where we predict $Y\in\reals^m$ given $X\in\reals^n$.
We want to design a predictor $g:\reals^n\to\reals^m$
so that $g(X) \sim Y$ in some statistical sense.
We first show that $g(X) = \Expect (Y|X)$ is the optimal predictor (or estimator)
in least-mean-square sense.

We define $g^\ast: \reals^n \to \reals^m$ where $g(x) = \Expect(Y|X=x)$. Then

\begin{eqnarray*}
\lefteqn{
\Expect \|g(X) - Y \|_2^2
= \Expect \|g(X) - g^\ast(X) + g^\ast(X) - Y \|_2^2
}
\\
&=&
	\Expect \|g(X) - g^\ast(X)\|_2^2
	+ \Expect \|g^\ast(X) - Y \|_2^2
	+ 2\Expect (g(X) - g^\ast(X))^T (g^\ast(X) - Y)
\\
&=&
	\Expect \|g(X) - g^\ast(X)\|_2^2
	+ \Expect \|g^\ast(X) - Y \|_2^2
	+ 2\Expect_X \Expect_Y \left( (g(X) - g^\ast(X))^T (g^\ast(X) - Y) |X \right)
\\
&=&
	\Expect \|g(X) - g^\ast(X)\|_2^2
	+ \Expect \|g^\ast(X) - Y \|_2^2
	+ 2\Expect_X (g(X) - g^\ast(X))^T \Expect_Y \left( g^\ast(X) - Y |X \right)
\\
&=&
	\Expect \|g(X) - g^\ast(X)\|_2^2
	+ \Expect \|g^\ast(X) - Y \|_2^2
	+ 2\Expect_X (g(X) - g^\ast(X))^T \left( g^\ast(X) - \Expect (Y|X) \right)
\\
&=&
	\Expect \|g(X) - g^\ast(X)\|_2^2
	+ \Expect \|g^\ast(X) - Y \|_2^2
	\geq \Expect \|g^\ast(X) - Y \|_2^2.
\end{eqnarray*}

Therefore $g^\ast(X)$ is the optimal predictor for $Y$ in the least-mean-square sense.


\section{Bias and Variance}

In \S\ref{sec:opt-predictor},
we proved that $g^\ast(X) = \Expect(Y|X)$ is the optimal predictor (or estimator)
in the least-mean-square sense.
However, unless we have the full knowledge of the joint probability distribution of $X$ and $Y$,
\ie, $p(X,Y)$,
or know $\Expect(Y|X=x)$ as a function of $x$,
it is not possible to obtain $g^\ast$.

Here we assume that we obtain the predictor for $Y$ given $X$
from a dataset $D$
where
\begin{equation}
D = \{(x_1, y_1), \ldots, (x_N, y_N)\} \subseteq \reals^n \times \reals^m.\footnote{
Note that strictly speaking, \dataset\ is \emph{not} a set
since the order of $(x_i, y_i) \in \reals^n \times \reals^m$ matters,
\ie, if the order is changed, we generally have different predictor,
and we are allowed to have identical data point.
Thus, we should say \dataset\ is a (ordered) list of points, $(x_i, y_i) \in \reals^n \times \reals^m$.
}
\end{equation}

Now suppose that we have a predictor $g(\cdot;D): \reals^n \to \reals^m$,
which depends on $D$.
Now let \dataset\ denote the random variable for this data set,
\ie,
\begin{equation}
\dataset = \{(X_1, Y_1), \ldots, (X_N, Y_N)\} \subseteq \reals^n \times \reals^m.
\end{equation}

Then the mean square error of this predictor can be decomposed as following.
\begin{eqnarray*}
\lefteqn{
\Expect_{X,Y,\dataset} \|g(X;\dataset) - Y\|_2^2
= \Expect_{X,Y,\dataset} \|g(X;\dataset) -g^\ast(X) + g^\ast(X)- Y\|_2^2
}
\\
&=&
	\Expect_{X,Y,\dataset} \|g(X;\dataset) -g^\ast(X) \|_2^2
	+ \Expect_{X,Y,\dataset} \|g^\ast(X)- Y\|_2^2
\\
&&
	+ 2 \Expect_{X,Y,\dataset} (g(X;\dataset) -g^\ast(X) )^T( g^\ast(X)- Y)
\\
&=&
	\Expect_{X,\dataset} \|g(X;\dataset) -g^\ast(X) \|_2^2
	+ \Expect_{X,Y} \|g^\ast(X)- Y\|_2^2
\\
&&
	+ 2 \Expect_{X,\dataset}  \Expect_Y \left( (g(X;\dataset) -g^\ast(X) )^T( g^\ast(X)- Y) |X,\dataset \right)
\\
&=&
	\Expect_{X,\dataset} \|g(X;\dataset) -g^\ast(X) \|_2^2
	+ \Expect_{X,Y} \|g^\ast(X)- Y\|_2^2
\\
&&
	+ 2 \Expect_{X,\dataset}  (g(X;\dataset) -g^\ast(X) )^T\Expect_Y \left( g^\ast(X)- Y |X,\dataset \right)
\\
&=&
	\Expect_{X,\dataset} \|g(X;\dataset) -g^\ast(X) \|_2^2
	+ \Expect_{X,Y} \|g^\ast(X)- Y\|_2^2
\\
&&
	+ 2 \Expect_{X,\dataset}  (g(X;\dataset) -g^\ast(X) )^T\Expect_Y \left( g^\ast(X)- Y |X \right)
\\
&=&
	\Expect_{X,\dataset} \|g(X;\dataset) -g^\ast(X) \|_2^2
	+ \Expect_{X,Y} \|g^\ast(X)- Y\|_2^2
\\
&&
	+ 2 \Expect_{X,\dataset}  (g(X;\dataset) -g^\ast(X) )^T( g^\ast(X)- \Expect_Y(Y|X))
\\
&=&
	\Expect_{X,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X; \dataset) + \Expect_\dataset g(X; \dataset) -g^\ast(X) \|_2^2
	+ \Expect_{X,Y} \|g^\ast(X)- Y\|_2^2
\\
&=&
	\Expect_{X,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X; \dataset)\|_2^2
	+ \Expect_{X,\dataset} \| \Expect_\dataset g(X; \dataset) -g^\ast(X) \|_2^2
	+ \Expect_{X,Y} \|g^\ast(X)- Y\|_2^2
\\
&&
	+ 2\Expect_{X,\dataset} (g(X;\dataset) - \Expect_\dataset g(X; \dataset))^T (\Expect_\dataset g(X; \dataset) -g^\ast(X) )
\\
&=&
	\Expect_{X,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X; \dataset)\|_2^2
	+ \Expect_{X} \| \Expect_\dataset g(X; \dataset) -g^\ast(X) \|_2^2
	+ \Expect_{X,Y} \|g^\ast(X)- Y\|_2^2
\\
&&
	+ 2\Expect_{X} \Expect_\dataset \left( (g(X;\dataset) - \Expect_\dataset g(X; \dataset))^T (\Expect_\dataset g(X; \dataset) -g^\ast(X) ) |X\right)
\\
&=&
	\Expect_{X,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X; \dataset)\|_2^2
	+ \Expect_{X} \| \Expect_\dataset g(X; \dataset) -g^\ast(X) \|_2^2
	+ \Expect_{X,Y} \|g^\ast(X)- Y\|_2^2
\\
&&
	+ 2\Expect_{X} \Expect_\dataset (g(X;\dataset) - \Expect_\dataset g(X; \dataset)|X)^T (\Expect_\dataset g(X; \dataset) -g^\ast(X))
\\
&=&
	\Expect_{X,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X; \dataset)\|_2^2
	+ \Expect_{X} \| \Expect_\dataset g(X; \dataset) -g^\ast(X) \|_2^2
	+ \Expect_{X,Y} \|g^\ast(X)- Y\|_2^2
\\
&&
	+ 2\Expect_{X} (\Expect_\dataset g(X;\dataset) - \Expect_\dataset g(X; \dataset)|X)^T (\Expect_\dataset g(X; \dataset) -g^\ast(X))
\\
&=&
	\Expect_{X,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X; \dataset)\|_2^2
	+ \Expect_{X} \| \Expect_\dataset g(X; \dataset) -g^\ast(X) \|_2^2
	+ \Expect_{X,Y} \|g^\ast(X)- Y\|_2^2.
\end{eqnarray*}

Note that we use the fact that $\Expect_\dataset g(X;\dataset)$ is a function of $X$ only
(hence does not depend on $X$).

In the last equation,
the first term is called the \emph{variance} since it is the expected value (with respect to $X$) of variance of the predictor, $g(X;\dataset)$,
with respect to the dataset, \dataset.
It represents the extent to which the prediction varies around its expected value.
The second term is the expected value of the square of the bias
where the bias is defined to be the difference between the expected value of prediction with respect to dataset and the optimal prediction.
The second term itself is sometimes called \emph{bias}.
The third term is called \emph{noise} since it is caused by the intrinsic noise residing in $Y$
which cannot be reduced even with the optimal predictor (in least-mean-square sense).

The following equation summarizes these three quantities.
\begin{eqnarray}
\label{eq:var-bias-noise}
\lefteqn{
\Expect_{X,Y,\dataset} \|g(X;\dataset) - Y\|_2^2
}
\\
&=&
\underbrace{\Expect_{X,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X; \dataset)\|_2^2}_\text{variance}
+
\underbrace{\Expect_{X} \| \Expect_\dataset g(X; \dataset) -g^\ast(X) \|_2^2}_\text{bias}
+
\underbrace{\Expect_{X,Y} \|g^\ast(X)- Y\|_2^2}_\text{noise}.
\nonumber
\end{eqnarray}

In general, we do not know the optimal predictor; if we knew, we would not need to train our in the first place.
Thus we can only estimate $g(X;\dataset) - \Expect_\dataset g(X;\dataset)$ and
$\Expect_\dataset g(X;\dataset)-Y$.
The mean square error can also be expressed in these two quantities as follows.

\begin{eqnarray*}
\lefteqn{
\Expect_{X,Y,\dataset} \|g(X;\dataset) - Y\|_2^2
=
\Expect_{X,Y,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X;\dataset) + \Expect_\dataset g(X;\dataset) -Y\|_2^2
}
\\
&=&
	\Expect_{X,Y,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X;\dataset)\|_2^2
	+ \Expect_{X,Y,\dataset} \|\Expect_\dataset g(X;\dataset) -Y\|_2^2
\\
&&
	+ 2\Expect_{X,Y,\dataset} (g(X;\dataset) - \Expect_\dataset g(X;\dataset))^T(\Expect_\dataset g(X;\dataset) -Y)
\\
&=&
	\Expect_{X,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X;\dataset)\|_2^2
	+ \Expect_{X,Y} \|\Expect_\dataset g(X;\dataset) -Y\|_2^2
\\
&&
	+ 2\Expect_{X,Y} \Expect_\dataset \left( (g(X;\dataset) - \Expect_\dataset g(X;\dataset))^T(\Expect_\dataset g(X;\dataset) -Y) |X,Y\right)
\\
&=&
	\Expect_{X,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X;\dataset)\|_2^2
	+ \Expect_{X,Y} \|\Expect_\dataset g(X;\dataset) -Y\|_2^2
\\
&&
	+ 2\Expect_{X,Y} \Expect_\dataset (g(X;\dataset) - \Expect_\dataset g(X;\dataset)|X,Y)^T(\Expect_\dataset g(X;\dataset) -Y)
\\
&=&
	\Expect_{X,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X;\dataset)\|_2^2
	+ \Expect_{X,Y} \|\Expect_\dataset g(X;\dataset) -Y\|_2^2
\\
&&
	+ 2\Expect_{X,Y} (\Expect_\dataset g(X;\dataset) - \Expect_\dataset g(X;\dataset))^T(\Expect_\dataset g(X;\dataset) -Y)
\\
&=&
{\Expect_{X,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X;\dataset)\|_2^2}
+ {\Expect_{X,Y} \|\Expect_\dataset g(X;\dataset) -Y\|_2^2}.
\end{eqnarray*}

Equating the last equation with (\ref{eq:var-bias-noise}) yields

\begin{equation}
\Expect_{X,Y,\dataset} \|g(X;\dataset) - Y\|_2^2
= \underbrace{\Expect_{X,\dataset} \|g(X;\dataset) - \Expect_\dataset g(X;\dataset)\|_2^2}_\text{variance}
+ \underbrace{\Expect_{X,Y} \|\Expect_\dataset g(X;\dataset) -Y\|_2^2}_\text{bias + noise}
\end{equation}

Therefore in reality, we can only obtain the sum of the bias and noise (not separately)
unless we know the quantity of the noise.


\newcommand{\numsamples}{\ensuremath{N}}
\newcommand{\sequence}[3]{\ensuremath{\{#1\}_{{#2}=1}^{#3}}}

\newcommand{\noise}{\ensuremath{\beta}}
\newcommand{\noiseset}{\ensuremath{\mathcal{B}}}

\newcommand{\prior}{\ensuremath{\alpha}}
\newcommand{\priorset}{\ensuremath{\mathcal{A}}}

\newcommand{\param}{\ensuremath{\theta}}
\newcommand{\paramset}{\Theta}
\newcommand{\paramdim}{l}

\newcommand{\bigX}{\tilde{X}}
\newcommand{\bigY}{\tilde{Y}}
\newcommand{\bigx}{\ensuremath{\tilde{x}}}
\newcommand{\bigy}{\ensuremath{\tilde{y}}}

\newcommand{\mles}[1]{{#1}_\mathrm{ML}}
\newcommand{\maps}[1]{{#1}_\mathrm{MAP}}

\section{Maximum Likelihood Estimation}

Suppose that $X \in\reals^n$ and $Y \in\reals^m$ are random variables
representing inputs (or independent variables or predictors or features)
and outputs (or dependent variables or responses).
We want to find a parameterized model to predict $Y$ from $X$.

We consider the parameter $\param\in\paramset$
where $\paramset\subset \reals^\paramdim$ is the set of all possible parameter values,
and the model, $g: \reals^n \times \reals^\paramdim \to \reals^m$, such that
$g(X; \param)$ is close to $Y$
in some statistical sense.

We further assume that the conditional probability of $Y$ given $g(X; \param)$ can be characterized by $\noise\in\noiseset$,
\ie, $p(Y|g(X;\param))$ is a function of $\noise$
where \noiseset\ is the set of all possible values for $\noise$.
To express that $p(Y|g(X;\param))$ is a function of $\noise$,
we will use a notation, $p(Y|X;\param, \noise)$, for $p(Y|g(X;\param))$.

Now suppose that we have observed \numsamples\ independent data sample, \sequence{(x_i, y_i)}{i}{\numsamples}
where $x_i\in\reals^n$ and $y_i\in\reals^m$.
We want to find $\param\in\paramset$ which maximizes the probability of this event,
\ie,
\begin{equation}
\label{eq:fuxy}
p\left((X_1, Y_1)=(x_1, y_1), \ldots (X_\numsamples, Y_\numsamples)=(x_\numsamples, y_\numsamples)\right)
\end{equation}

For notational convenience, we define two random variables, $\bigX\in\reals^{n\times \numsamples}$
and $\bigY\in\reals^{m\times \numsamples}$, such that
\begin{equation}
\bigX = \begin{my-matrix}{ccc}
X_1 &\ldots &X_\numsamples
\end{my-matrix}
\end{equation}
and
\begin{equation}
\bigY = \begin{my-matrix}{ccc}
Y_1 &\ldots &Y_\numsamples
\end{my-matrix}.
\end{equation}
We also defined $\bigx\in\reals^{n\times \numsamples}$ and $\bigy\in\reals^{m\times \numsamples}$, such that
\begin{equation}
\bigx = \begin{my-matrix}{ccc}
x_1 &\ldots &x_\numsamples
\end{my-matrix}
\end{equation}
and
\begin{equation}
\bigy = \begin{my-matrix}{ccc}
y_1 &\ldots &y_\numsamples
\end{my-matrix}.
\end{equation}

Then (\ref{eq:fuxy}) becomes
\begin{equation}
p(\bigX = \bigx, \bigY=\bigy).
\end{equation}
Bayes's theorem and the independence among $(X_i, Y_i)$ imply
\begin{eqnarray*}
\lefteqn{
p(\bigX = \bigx, \bigY=\bigy)
= \prod_{i=1}^\numsamples p(X_i=x_i, Y_i=y_i)
}
\\
&=&
\prod_{i=1}^\numsamples p(X_i=x_i| Y_i=y_i; \param, \noise) p(X_i=x_i)
\\
&\propto&
\prod_{i=1}^\numsamples p(X_i=x_i| Y_i=y_i; \param, \noise).
\end{eqnarray*}
Here $p(X_i=x_i| Y_i=y_i; \param, \noise)$ is called \emph{likelihood function}.


The maximum likelihood estimation (MLE) (or learning) is to find $\param$ (and $\noise$) which maximizes this probability.
Since the log function is a strictly increasing function, taking log on this probability does give us the same results when maximizing it.
\begin{equation}
\label{eq:log-likelihood}
\log p(\bigX = \bigx, \bigY=\bigy)
= \sum_{i=1}^\numsamples \log p(X_i=x_i| Y_i=y_i; \param, \noise) + \text{constant}
\end{equation}
In many cases, this log form is preferable due to its numerical property.


The optimal value of \param\ for the maximum likelihood estimation (when $\noise$ is fixed) can be written as
\begin{equation}
\label{eq:mle-solution}
\mles{\param}(\noise) = \argmax_{\param\in\paramset} \sum_{i=1}^\numsamples \log p(X_i=x_i| Y_i=y_i; \param, \noise).
\end{equation}

Note that the solution is a function of $\noise$ when it is fixed.
We sometimes want to find the optimal value for $\noise$, too.
In this case,
we have

\begin{equation}
\label{eq:mle-solution-1}
(\mles{\param}, \mles{\noise}) = \argmax_{(\param, \noise)\in\paramset\times \noiseset} \sum_{i=1}^\numsamples \log p(X_i=x_i| Y_i=y_i; \param, \noise).
\end{equation}

Note that both of these are point estimation, \ie,
we want to find one value for \param\ (and \noise) to maximize the log likelihood function.

Now the MLE model is given by $g(\cdot; \mles{\param}): \reals^n \to \reals^m$,
\ie, given $x\in\reals^n$, we can predict $y\in\reals^m$ by $g(x; \mles{\param})$.

\section{Maximum a Posteriori Estimation}

In MLE, we regard \param\ as a deterministic variable,
which is called Frequentist perspective.

However, we sometimes have prior knowledge of the distribution of \param\ (or belief about \param).
In this situation, we want to find probability distribution of \param\ after observing some evidence,
\eg, the \numsamples\ data sample we have observed, \sequence{(x_i, y_i)}{i}{\numsamples}.

The natural way of finding the distribution of \param\ after observing this evidence
is to evaluate the condition probability of \param\ given \bigx\ and \bigy, \ie,
\begin{equation}
p(\param|\bigX=\bigx, \bigY=\bigy).
\end{equation}
Note that \param\ is considered to be a \emph{random} variable
unlike in MLE case.

Here we introduce a new parameter $\prior\in\priorset$ that characterizes the distribution of \param\
where \priorset\ is the set of all the possible values of \prior.

Since the data samples are assumed to be independent,
the Bayes' theorem implies that
\begin{eqnarray*}
\lefteqn{
p(\param|\bigX=\bigx, \bigY=\bigy; \prior, \noise)
=
p(\bigX=\bigx, \bigY=\bigy| \param; \prior, \noise)
p(\param; \prior, \noise)
/ p(\bigX=\bigx, \bigY=\bigy)
}
\\
&=&
p(\bigX=\bigx, \bigY=\bigy| \param; \noise) p(\param; \prior)
/ p(\bigX=\bigx, \bigY=\bigy)
\\
&\propto&
p(\bigX=\bigx, \bigY=\bigy| \param; \noise) p(\param; \prior)
\\
&=&
p(\param; \prior)
\prod_{i=1}^\numsamples p(X_i=x_i, Y_i=y_i| \param; \noise)
\\
&=&
p(\param; \prior)
\prod_{i=1}^\numsamples p(Y_i=y_i|X_i=x_i,  \param; \noise)  p(X_i=x_i| \param; \noise)
\\
&=&
p(\param; \prior)
\prod_{i=1}^\numsamples p(Y_i=y_i|X_i=x_i,  \param; \noise)  p(X_i=x_i)
\\
&\propto&
p(\param; \prior)
\prod_{i=1}^\numsamples p(Y_i=y_i|X_i=x_i,  \param; \noise)
\end{eqnarray*}

The maximum a posteriori (MAP) estimation is to find \param\ (when \noise\ is fixed)
which maximizes this posteriori probability.
Thus, the MAP solution can be expressed as
\begin{equation}
\label{eq:map-solution}
\maps{\param}(\prior, \noise) = \argmax_{\param\in\paramset}
\left(
\log p(\param; \prior)
+
\sum_{i=1}^\numsamples \log p(Y_i=y_i|X_i=x_i,  \param; \noise)
\right)
\end{equation}
where $p(Y_i=y_i|X_i=x_i, \param; \noise)$ is called \emph{likelihood function}.

Note the difference between the likelihood function, $p(Y_i=y_i|X_i=x_i;  \param, \noise)$, in (\ref{eq:mle-solution})
and the likelihood function, $p(Y_i=y_i|X_i=x_i,  \param; \noise)$, in (\ref{eq:map-solution})
where \param\ in (\ref{eq:mle-solution}) is an optimization variable
and \param\ in (\ref{eq:map-solution}) is a variable for a random variable.

Now the MAP model is given by $g(\cdot; \maps{\param}): \reals^n \to \reals^m$,
\ie, given $x\in\reals^n$, we can predict $y\in\reals^m$ by $g(x; \maps{\param})$.


\section{Bayesian prior update}

Note that both MLE and MAP estimation is a point estimation,
\ie, to find one solution that maximizes some probability.

However, the posterior probability $p(\param|\bigX=\bigx, \bigY=\bigy; \prior, \noise)$ can be used to update the prior probability.

In Bayesian probability theory,
if the posterior distributions are in the same probability distribution family
as the prior probability distribution,
the prior and posterior are then called \emph{conjugate distributions},
and the prior is called a \emph{conjugate prior} for the likelihood function.

In this case, we can update the prior by updating \prior.
Suppose that we have initial prior, $\prior^{(0)} \in \priorset$.
After we observing first data samples, $(\bigx^{(1)}, \bigy^{(1)})$,
we evaluate the posterior probability $p(\param|\bigX=\bigx^{(1)}, \bigY=\bigy^{(1)}; \prior^{(0)}, \noise)$
which can be characterized by some $\param^+$ due to conjugate distribution assumption.
We let $\prior^{(1)}$ be this updated parameter.
We can repeat this process every time we observe new set of data samples.
This process can be expressed as
\begin{equation}
\prior^{(0)}
\xrightarrow{\bigx^{(1)}, \bigy^{(1)}}
\prior^{(1)}
\xrightarrow{\bigx^{(2)}, \bigy^{(2)}}
\prior^{(2)}
\xrightarrow{\bigx^{(2)}, \bigy^{(2)}}
\prior^{(3)}
\cdots
\end{equation}

We can see this process as the one similar to what happens in our brain.
A simplified version of explaining human learning process is
to update its prior knowledge whenever it observes new evidence.
For example,
if one has observed that when it rains,
the temperature is high,
her prior knowledge is that
\begin{equation}
\text{rain} \rightarrow \text{high temperature}.
\end{equation}
However, if she experiences a rainy day with low temperature,
her knowledge is updated as something like
\begin{equation}
\text{rain} \rightarrow \left\{\begin{array}{ll}
\text{high temperature} &\text{with probability}\ 0.9
\\
\text{low temperature} &\text{with probability}\ 0.1
\end{array}\right.
\end{equation}
Now this becomes her new prior.
If she observes more rainy cold days,
her knowledge is updated as something like
\begin{equation}
\text{rain} \rightarrow \left\{\begin{array}{ll}
\text{high temperature} &\text{with probability}\ 0.7
\\
\text{low temperature} &\text{with probability}\ 0.3
\end{array}\right.
\end{equation}

This analogy tells why the prior in Bayesian statistics
is sometimes called Bayesian belief.
This prior belief is something that can be constantly updated with new evidence.


\section{Predictive Distribution}

If \param\ is fixed,
the probability of $y\in\reals^m$ given $x\in\reals^n$,
$p(Y=y|X=x; \param, \noise)$,
is solely characterized by $\noise\in\noiseset$.
However, if we regard \param\ as a random variable with distribution characterized by \prior,
the probability of $y\in\reals^m$ given $x\in\reals^n$
can be evaluated by
\begin{eqnarray*}
\lefteqn{
p(Y=y|X=x; \prior, \noise)
=
\int_{\param\in\paramset} p(Y=y, \param|X=x; \prior, \noise) d\param
}
\\
&=&
\int_{\param\in\paramset} p(Y=y| X=x,\param; \prior, \noise) p(\param|X=x; \prior, \noise) d\param
\\
&=&
\int_{\param\in\paramset} p(Y=y| X=x,\param; \noise) p(\param; \prior) d\param,
\end{eqnarray*}
\ie,
\begin{equation}
\label{eq:pred-dist}
p(Y=y|X=x; \prior, \noise) = \int_{\param\in\paramset} p(Y=y| X=x,\param; \noise) p(\param; \prior) d\param.
\end{equation}

This is called predictive distribution.
This Bayesian statistical predictor,
if (\ref{eq:pred-dist}) can be efficiently evaluated,
not only gives the point estimation, \eg, by mean or mode,
but also the distribution of the output.
One advantage of this approach is that
we can evaluate the confidence interval

