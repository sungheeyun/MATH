

\chapter{Statistics Basics}

\section{Correlation coefficients}

The correlation coefficients of two random variables, $X$ and $Y$, is defined by

\begin{equation}
\rho_{X,Y} = \frac{\Expect (X-\mu_X)(Y-\mu_Y)} {\sqrt{\Expect (X-\mu_X)^2 \Expect(Y-\mu_Y)^2}}
\end{equation}


\section{Transformation of a random variable via a function}

\subsection{Scale random variable}

Assume two random variables, $X\in\reals$ and $Y\in\reals$, which are related by
a function $g:\reals \to \reals \in C^{1}$
such that
\begin{equation}
\label{eq:g8cx}
Y = g(X).
\end{equation}

Now let's derive an equation for the probability density function (PDF) of $Y$
given the PDF of $X$, $f_X:\reals\to\preals$.

The definition of cumulative distribution function (CDF) of $Y$ implies that
\begin{equation}
\label{eq:gnpz}
    F_Y(y) = \Prob\{Y \leq y\} = \Prob\{g(X) \leq y\}
\end{equation}
for any $y\in\reals$.

Now if we assume that $g$ is a strictly increasing function, it has its inverse function $g^{-1}: g(\reals) \to \reals$
and
(\ref{eq:gnpz}) becomes
\[
    F_Y(y) = \Prob\{g(X) \leq y\}
    = \Prob\{X \leq g^{-1}(y)\}
    = F_X(g^{-1}(y))
\]
for any $y\in g(\reals)$.
Thus, we can differentiate both sides to have
\begin{equation}
\label{eq:gnpz-1}
    f_Y(y) = \frac{d}{dy} F_Y(y)
    = \frac{d}{dy} F_X(g^{-1}(y))
    = f_X(g^{-1}(y)) \frac{d}{dy} g^{-1}(y)
    = \frac{1}{g'(g^{-1}(y))} f_X(g^{-1}(y)),
\end{equation}
since (\ref{theorem:chain-rule}) implies that
\begin{equation}
1
= \frac{d}{dx} g(g^{-1}(x))
= g'(g^{-1}(x)) \frac{d}{dx} g^{-1}(x),
\end{equation}
\ie, the derivative of the inverse function
is the inverse of the derivative of the original function.

Now if we assume that $g$ is a strictly decreasing function, we have
\[
    F_Y(y) = \Prob\{g(X) \leq y\}
    = \Prob\{x \geq g^{-1}(y)\}
    = 1 - F_X(g^{-1}(y))  + \Prob\{x = g^{-1}(y)\},
\]
and
\begin{equation}
\label{eq:gnpz-2}
    f_Y(y) = \frac{d}{dy} F_Y(y)
    = - \frac{d}{dy} F_X(g^{-1}(y))
    = - f_X(g^{-1}(y)) \frac{d}{dy} g^{-1}(y)
    = - \frac{1}{g'(g^{-1}(y))} f_X(g^{-1}(y)).
\end{equation}
Since $g'(y)>0$ for a strictly increasing function,
and $g'(y)<0$ for a strictly decreasing function,
(\ref{eq:gnpz-1}) and (\ref{eq:gnpz-2}) imply
\begin{equation}
\label{eq:pdf-rv-transform-1}
    f_Y(y) = \frac{1}{|g'(g^{-1}(y))|} f_X(g^{-1}(y))
\end{equation}
for both cases.

Now consider a general function, $g$, \ie, not necessarily a monotonic function.
Suppose that $y\in g(\reals)$.
Then for every $x\in\reals$ such that $f(x) = y$,
if $f'(x)\neq0$, then $f(x)$ is locally strictly monotonic,
\ie, there exists $\delta>0$ such that $f(x)$ is strictly monotonic for $x\in(x-\delta, x + \delta)$,
hence (\ref{eq:pdf-rv-transform-1}) holds for such $x$.

The probability around $y$ is a summation of the probabilities around such points,
\ie, the probabilities around all $x$ such that $f(x) =y$ and $f'(x)\neq0$.
Therefore, we have
\begin{equation}
\label{eq:pdf-rv-transform}
    f_Y(y) = \sum_{x:g(x)=y}\frac{1}{|g'(x)|} f_X(x).
\end{equation}

There is another way to derive the same equation (in a less strict way)
which helps get more insight.
Let's again suppose that $g$ is a strictly increasing function.
Then consider the probability that $X$ lies in $(x, x + \Delta x)$.
The probability should be the same as $Y$ lies in $(y, y + \Delta y)$
where $y=g(x)$ and $\Delta y = g(x+\Delta x) - g(x)$,
\ie,
\begin{eqnarray*}
\lefteqn{
f_X(x) \Delta x
\approx
\int_{x}^{x + \Delta x} f_X(x) \, dx
=
\Prob\{x\leq X\leq x + \Delta x\}
}
\\
&=&
\Prob\{y\leq Y\leq y + \Delta y\}
=
\int_{y}^{y + \Delta y} f_Y(y) \, dy
\approx
f_Y(y) \Delta y.
\end{eqnarray*}
The approximation becomes the equality when $\Delta x$ goes to $0$.
Therefore we have
\begin{equation}
f_Y(y)
= \lim_{\Delta x \to 0}\frac{\Delta x}{\Delta y} f_X(x)
= \frac{1}{\lim_{\Delta x \to 0} \frac{g(x+\Delta x)-g(x)}{\Delta x}} f_X(x)
= \frac{1}{g'(x)} f_X(x),
\end{equation}
which is equivalent to (\ref{eq:gnpz-1}).
Following the very same argument as before will lead to (\ref{eq:pdf-rv-transform}),
\ie, applying the same method to strictly decreasing case, \etc\



\subsection{Multivariate random variable}

Assume two random variables, $X\in\reals^n$ and $Y\in\reals^n$, which are related by
a function $g:\reals^n \to \reals^n$
such that
\begin{equation}
\label{eq:wnux-1}
Y = g(X).
\end{equation}
Assume that $g$ is differentiable everywhere in $\dom g$,
\ie, the Jacobian matrix defined in (\ref{eq:jacobian-matrix}) exists for all $x \in \dom g$.

XXX

Now we suppose that $g$ is a strictly increasing and invertible function.
Then the definition of cumulative distribution function (CDF) of $Y$ implies that
\begin{equation}
\label{eq:wnux-2}
    F_Y(y) = \Prob\{Y \preceq y\} = \Prob\{g(X) \preceq y\}
\end{equation}
for any $y\in\reals^n$ where $\preceq$ is the componentwise inequality.
Since $g$ is an invertible and stricly increasing function, the inverse function $g^{-1}: g(\reals^n) \to \reals^n$
exists and 
(\ref{eq:wnux-2}) becomes
\[
    F_Y(y) = \Prob\{g(X) \preceq y\}
    = \Prob\{X \preceq g^{-1}(y)\}
    = F_X(g^{-1}(y))
\]
for any $y\in g(\reals^n)$.
Thus, we can differentiate both sides to have
\begin{equation}
\label{eq:gnpz-1}
    f_Y(y) = \frac{d}{dy} F_Y(y)
    = \frac{d}{dy} F_X(g^{-1}(y))
    = f_X(g^{-1}(y)) \frac{d}{dy} g^{-1}(y)
    = \frac{1}{g'(g^{-1}(y))} f_X(g^{-1}(y)),
\end{equation}
since (\ref{theorem:chain-rule}) implies that
\begin{equation}
1
= \frac{d}{dx} g(g^{-1}(x))
= g'(g^{-1}(x)) \frac{d}{dx} g^{-1}(x),
\end{equation}
\ie, the derivative of the inverse function
is the inverse of the derivative of the original function.

Now if we assume that $g$ is a strictly decreasing function, we have
\[
    F_Y(y) = \Prob\{g(X) \leq y\}
    = \Prob\{x \geq g^{-1}(y)\}
    = 1 - F_X(g^{-1}(y))  + \Prob\{x = g^{-1}(y)\},
\]
and
\begin{equation}
\label{eq:gnpz-2}
    f_Y(y) = \frac{d}{dy} F_Y(y)
    = - \frac{d}{dy} F_X(g^{-1}(y))
    = - f_X(g^{-1}(y)) \frac{d}{dy} g^{-1}(y)
    = - \frac{1}{g'(g^{-1}(y))} f_X(g^{-1}(y)).
\end{equation}
Since $g'(y)>0$ for a strictly increasing function,
and $g'(y)<0$ for a strictly decreasing function,
(\ref{eq:gnpz-1}) and (\ref{eq:gnpz-2}) imply
\begin{equation}
\label{eq:pdf-rv-transform-1}
    f_Y(y) = \frac{1}{|g'(g^{-1}(y))|} f_X(g^{-1}(y))
\end{equation}
for both cases.

Now consider a general function, $g$, \ie, not necessarily a monotonic function.
Suppose that $y\in g(\reals)$.
Then for every $x\in\reals$ such that $f(x) = y$,
if $f'(x)\neq0$, then $f(x)$ is locally strictly monotonic,
\ie, there exists $\delta>0$ such that $f(x)$ is strictly monotonic for $x\in(x-\delta, x + \delta)$,
hence (\ref{eq:pdf-rv-transform-1}) holds for such $x$.

The probability around $y$ is a summation of the probabilities around such points,
\ie, the probabilities around all $x$ such that $f(x) =y$ and $f'(x)\neq0$.
Therefore, we have
\begin{equation}
\label{eq:pdf-rv-transform}
    f_Y(y) = \sum_{x:g(x)=y}\frac{1}{|g'(x)|} f_X(x).
\end{equation}

There is another way to derive the same equation (in a less strict way)
which helps get more insight.
Let's again suppose that $g$ is a strictly increasing function.
Then consider the probability that $X$ lies in $(x, x + \Delta x)$.
The probability should be the same as $Y$ lies in $(y, y + \Delta y)$
where $y=g(x)$ and $\Delta y = g(x+\Delta x) - g(x)$,
\ie,
\begin{eqnarray*}
\lefteqn{
f_X(x) \Delta x
\approx
\int_{x}^{x + \Delta x} f_X(x) \, dx
=
\Prob\{x\leq X\leq x + \Delta x\}
}
\\
&=&
\Prob\{y\leq Y\leq y + \Delta y\}
=
\int_{y}^{y + \Delta y} f_Y(y) \, dy
\approx
f_Y(y) \Delta y.
\end{eqnarray*}
The approximation becomes the equality when $\Delta x$ goes to $0$.
Therefore we have
\begin{equation}
f_Y(y)
= \lim_{\Delta x \to 0}\frac{\Delta x}{\Delta y} f_X(x)
= \frac{1}{\lim_{\Delta x \to 0} \frac{g(x+\Delta x)-g(x)}{\Delta x}} f_X(x)
= \frac{1}{g'(x)} f_X(x),
\end{equation}
which is equivalent to (\ref{eq:gnpz-1}).
Following the very same argument as before will lead to (\ref{eq:pdf-rv-transform}),
\ie, applying the same method to strictly decreasing case, \etc\


\subsection{Data Examples}

Suppose that we have $n$ random variables, $X_1$, \ldots, $X_n$
and they are independent and identically distributed Gaussian, $\mathcal{N}(0,1)$.
Then assume that a random variable, $Y$, is the sum of the $X_i$'s,
\ie,
\begin{equation}
Y = \sum_{i=1}^n X_i = X_1 + \cdots + X_n
\end{equation}

Then the covariance of $X_i$ and $Y$ for each $i$ is
\begin{equation}
\mathbf{Cov}(X_i,Y) = \Expect (X_i - \Expect{X_i})( Y - \Expect Y) = \Expect\left( \sum_{j=1}^n X_iX_j\right) = 1
\end{equation}
and
the variance of $Y$ is
\begin{equation}
\mathbf{Var}(Y) = \Expect (Y - \Expect Y)^2 = \Expect\left( \sum_{j=1}^n X_iX_j\right)^2
= \sum_{i=1}^n \Expect X_i^2 = n.
\end{equation}

Hence, the correlation coefficient of $X_i$ and $Y$ for each $i$ is
\begin{equation}
\rho_{X_i,Y} = \mathbf{Cov}(X_i,Y) / \sqrt{\mathbf{Var}X_i \mathbf{Var} Y} = 1 / \sqrt(n).
\end{equation}

\emph{Therefore $Y$ has clear relation with $X_i$'s, but each correlation coefficient can be arbitrarily small as $n$ grows!}




\chapter{Various distributions}

\section{Log-normal distribution}

 We say $Y$ is log-normally distributed, if, for $X\sim\mathcal{N}(\mu_X,\sigma_X^2)$,
 \begin{equation}
 Y = \exp(X).
 \end{equation}

Then (\ref{eq:pdf-transform}) implies that
\begin{eqnarray}
f_Y(y) &=& \frac{1}{\exp(\log(y))} \cdot \frac{1}{\sqrt{2\pi} \sigma_X}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right)
\nonumber
\\
&=& \frac{1}{\sqrt{2\pi} \sigma_X y}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right).
\label{eq:log-normal-pdf}
\end{eqnarray}


\subsection{Some statistics}

The definition of the expected value implies
\begin{eqnarray*}
\lefteqn{
\Expect Y = \int_{0}^{\infty} y f_Y(y) \, dy
= \int_{0}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right) dy
}
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x-\mu_X)^2}{2\sigma_X^2}\right) \exp(x) \, dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x-\mu_X)^2}{2\sigma_X^2}+x\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{x^2 - 2(\mu_X +\sigma_X^2)x + \mu_X^2}{2\sigma_X^2}\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +\sigma_X^2))^2 + \mu_X^2 -(\mu_X+\sigma_X^2)^2}{2\sigma_X^2}\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +\sigma_X^2))^2 - 2\mu_X\sigma_X^2 - \sigma_X^4}{2\sigma_X^2}\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +\sigma_X^2))^2 - \sigma_X^2(2\mu_X+ \sigma_X^2)}{2\sigma_X^2}\right) dx
\\
&=&
\exp\left(\frac{2\mu_X+ \sigma_X^2}{2}\right)
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +\sigma_X^2))^2 }{2\sigma_X^2}\right) dx
\\
&=&
\exp\left(\frac{2\mu_X+ \sigma_X^2}{2}\right)
\end{eqnarray*}
since $dy = \exp(x) dx$
and $\frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +\sigma_X^2))^2 }{2\sigma_X^2}\right)$
is the PDF of a random variable $\sim$ $\mathcal{N}(\mu_X+\sigma_X^2,\sigma_X^2)$,
thus
\begin{equation}
\label{eq:log-normal-mean}
\mu_Y
= \Expect Y = \exp\left(\frac{2\mu_X+ \sigma_X^2}{2}\right).
\end{equation}

Similarly,
\begin{eqnarray*}
\lefteqn{
\Expect Y^2 = \int_{0}^{\infty} y^2 f_Y(y) \, dy
= \int_{0}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  y \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right) dy
}
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp(x) \exp\left(-\frac{(x-\mu_X)^2}{2\sigma_X^2}\right) \exp(x) \, dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x-\mu_X)^2}{2\sigma_X^2}+2x\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{x^2 - 2(\mu_X +2\sigma_X^2)x + \mu_X^2}{2\sigma_X^2}\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +2\sigma_X^2))^2 + \mu_X^2 -(\mu_X+2\sigma_X^2)^2}{2\sigma_X^2}\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +2\sigma_X^2))^2 - 4\mu_X\sigma_X^2 - 4\sigma_X^4}{2\sigma_X^2}\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +2\sigma_X^2))^2 - 4\sigma_X^2(\mu_X+ \sigma_X^2)}{2\sigma_X^2}\right) dx
\\
&=&
\exp\left({2(\mu_X+ \sigma_X^2)}\right)
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +2\sigma_X^2))^2 }{2\sigma_X^2}\right) dx
\\
&=&
\exp\left({2(\mu_X+ \sigma_X^2)}\right),
\end{eqnarray*}
thus
\begin{equation}
\label{eq:log-normal-var}
\sigma_Y^2 =
\Var(Y) = \Expect Y^2 - (\Expect Y)^2 = \exp(2(\mu_X+\sigma_X^2)) - \exp(2\mu_X+\sigma_X^2)
= (\exp(\sigma_X^2)-1) \exp(2\mu_X+\sigma_X^2)).
\end{equation}

Note that (\ref{eq:log-normal-mean}) implies that
\begin{equation}
\mu_Y^2
= \exp(2\mu_X+\sigma_X^2)),
\end{equation}
hence
\begin{equation}
\sigma_Y^2
= (\exp(\sigma_X^2)-1) \mu_Y^2.
\end{equation}
This multiplicative dependency of the standard deviation on the expected value
is attributed to the fact that $\log(Y) \sim \mathcal{N}(\mu_X,\sigma_X^2)$,
\ie,
the log-scale of $Y$ follows the normal distribution.

Now if we differentiate the PDF with respect to $y$,
(\ref{eq:log-normal-pdf}) implies that
\begin{eqnarray*}
\lefteqn{
\frac{d}{dy} f_Y(y)
= \frac{d}{dy} \left(\frac{1}{\sqrt{2\pi} \sigma_X y}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right)\right)
}
\\
&=&
-\frac{1}{\sqrt{2\pi} \sigma_X y^2}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right)
\\
&&
+
\frac{1}{\sqrt{2\pi} \sigma_X y}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right)
\left(-\frac{(\log(y)-\mu_X)}{\sigma_X^2}\right)
\frac{1}{y}
\\
&=&
-\frac{1}{\sqrt{2\pi} \sigma_X y^2}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right)
\left(1+\frac{(\log(y)-\mu_X)}{\sigma_X^2} \right)
\\
&=&
-\frac{1}{\sqrt{2\pi} \sigma_X^3 y^2}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right)
\left(\log(y)-(\mu_X-\sigma_X^2) \right).
\end{eqnarray*}
Equating the derivative to zero yields
\begin{equation}
y = \exp(\mu_X-\sigma_X^2),
\end{equation}
which is the mode of $Y$.


\subsection{Parameter estimation}

Now assume that we have a log-normally distributed random variable, $Y\in\ppreals$,
with $\mu_Y$ and $\sigma_Y^2$ as its mean and variance.
We derived the parameters of the source distribution, $\mu_X$ and $\sigma_X$.

The two equations, (\ref{eq:log-normal-mean}) and (\ref{eq:log-normal-var}), imply
\begin{eqnarray*}
\mu_Y &=& \exp(\mu_X+\sigma_X^2/2),
\\
\sigma_Y^2 &=& (\exp(\sigma_X^2)-1)\exp(2\mu_X+\sigma_X^2) = (\exp(\sigma_X^2)-1) \mu_Y^2,
\end{eqnarray*}
thus
\begin{eqnarray*}
\sigma_X^2 &=& \log(1+{\sigma_Y^2}/{\mu_Y^2}),
\\
\mu_X &=& \log(\mu_Y) - \sigma_X^2/2 = \log(\mu_Y) - \log(1+{\sigma_Y^2}/{\mu_Y^2})/2
= \frac{1}{2} \log\left(\frac{\mu_Y^2}{1+{\sigma_Y^2}/{\mu_Y^2}}\right).
\end{eqnarray*}


\chapter{Bayesian Statistics}

\section{Bayesian Theorem}

Suppose that $A$ and $B$ are two events with $\prob{B} \neq 0$. Then
\begin{equation}
\label{eq:bayes-theorem}
\condprob{A}{B} = \frac{\condprob{B}{A} \prob{A}}{\prob{B}}.
\end{equation}
This is called \emph{Bayesian theorem}.

\section{Bayesian Inference}

\begin{equation}
p(\theta|X)
= \frac{p(X|\theta) p(\theta)}{ p(X)}
= \frac{p(X|\theta) p(\theta)}{ \int p(X|\theta) p(\theta) d \theta }
\propto
p(X|\theta) p(\theta)
=
\mathrm{likelihood} \times \mathrm{prior}
\end{equation}


\section{Conjugate prior}

\subsection{Bernoulli distribution}

For Bernoulli distribution,
the conjugate prior is the beta distribution.
\begin{equation}
p(\theta) = \frac{1}{B(a,b)} \theta^{a-1} (1-\theta)^{b-1}
\end{equation}

Then the posterior probability can be expressed as
\begin{eqnarray*}
p(\theta|X)
&\propto& p(X|\theta) p(\theta)
\propto \left(\prod_{i=1}^n \theta^{I(x_i=1)} (1-\theta)^{I(x_i=0)} \right) \theta^{a-1} (1-\theta)^{b-1}
\\
&=&
\theta^{k+a-1}(1-\theta)^{n-k+b-1}
\end{eqnarray*}
where $k$ is the number of $1$s in $X$.
Thus $p(\theta|X) \sim \mathrm{Beta}(k+a, n-k+b)$, \ie,
\begin{equation}
p(\theta|X) = \frac{1}{B(k+a,n-k+b)} \theta^{k+a-1}(1-\theta)^{n-k+b-1}.
\end{equation}






\subsection{Gaussian distribution}

\newcommand{\gauss}{\mathcal{N}}

Suppose that $X \sim \gauss(\mu, \tau^{-1})$ and $\mu \sim \gauss(m,\lambda^{-1})$.
Then the posterior distribution is
\begin{equation}
p(\mu|X) \sim \gauss\left(
\frac{\tau \sum_{i=1}^n x_i + \lambda m}{n\tau + \lambda},
(n\tau + \lambda)^{-1}
\right)
\end{equation}





\chapter{Information Theory}

\section{Basics}


\subsection{Entropy}

\subsection{Mutual Information}

The mutual information (MI) is defined by

\begin{equation}
I(X;Y) = \Expect \log \frac{f_{X,Y}(X,Y)}{f_{X}(X)f_{Y}(Y)}
= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \log \frac{f_{X,Y}(x,y)}{f_{X}(x)f_{Y}(y)} \, dx dy
\end{equation}


\subsection{Relative Entropy (Kullback–Leibler divergence)}

The relative entropy of the two random distributions, $p$ and $q$, is defined by
\begin{equation}
\label{eq:def-rel-ent}
D(p\|q) = \Expect_p \log\left(\frac{p(X)}{q(X)}\right).
\end{equation}

The relative entropy represents the difference measure between the two distributions.
Unlike the mutual information, relative entropy is asymmetric, \ie,
in general, $D(p\|q) \neq D(q\|p)$.
It is always nonnegative quantity since the Jensen's inequality implies that
\begin{equation}
-D(p\|q) = \Expect_p \log\left(\frac{q(X)}{p(X)}\right)
\leq \log\left(\Expect_p \left(\frac{q(X)}{p(X)}\right)\right)
= \log\left(\int_{-\infty}^\infty q(x)\, dx \right) = 0.
\end{equation}

The relative entropy can be rewritten as
\begin{equation}
D(p\|q) = -\Expect_p \log q(X) + \Expect_p \log p(X)
= -\Expect_p \log q(X) - H(p)
\end{equation}
where the first term, $-\Expect_p \log q(X)$ is called the \emph{cross-entropy} of $p$ and $q$.

