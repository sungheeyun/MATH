\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format

\input{/Users/sunyun/mytex/mydefs}
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ...
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex
\usepackage{amssymb}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    %urlcolor=cyan,
    urlcolor=blue,
}

%SetFonts

%SetFonts

\newcommand{\feasibleset}{\mathcal{F}}
\newcommand{\optsolset}{\mathcal{X}^\ast}
\newcommand{\grad}{\nabla}
\newcommand{\possemidefset}[1]{\mathcal{S}_+^{#1}}
\newcommand{\posdefset}[1]{\mathcal{S}_{++}^{#1}}
\newcommand{\covmat}[1]{{\Sigma}_{#1}}


\title{Optimization in finance for Jisun}
\author{Sunghee Yun}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Linear Algebra}
\subsection{Eigenvalues}

\subsubsection{Basic definitions}

Given a square matrix $A\in\reals^{n\times n}$,
if there exist $\lambda \in \complexes$ and nonzero $v \in \complexes^n$ such that
\begin{equation}
        A v = \lambda v
\end{equation}
then $\lambda$ is called an eigenvalue of $A$ and $v$ is called an eigenvector associated with $\lambda$.

If there exist $n$ linearly independent eigenvectors, we have
\begin{equation}
A \begin{my-matrix}{ccc} v_1 & \cdots & v_n \end{my-matrix}
= \begin{my-matrix}{ccc} v_1 & \cdots & v_n \end{my-matrix} \diag(\lambda_1,\ldots,\lambda_n)
\end{equation}
or
\begin{equation}
\label{eq:1}
A V = V \Lambda
\end{equation}
where
\begin{equation}
V = \begin{my-matrix}{ccc} v_1 & \cdots & v_n \end{my-matrix}
\in\complexes^{n\times n}
\end{equation}
and
\begin{equation}
\Lambda = \diag(\lambda_1,\ldots,\lambda_n)
= \begin{my-matrix}{cccc}
\lambda_1 & 0 & \cdots & 0
\\
0 & \lambda_2 & \cdots & 0
\\
\vdots & \vdots & \ddots & \vdots
\\
0 & 0 & \cdots & \lambda_n
\end{my-matrix}
\in\complexes^{n\times n}.
\end{equation}
In this case, $A$ is said to be diagonalizable.

Since $V$ is nonsingular, \ie, invertible, we can rewrite (\ref{eq:1}) as
\begin{equation}
\label{eq:2}
A = V \Lambda V^{-1} \Leftrightarrow V^{-1} A V = \Lambda.
\end{equation}


\subsubsection{Symmetric matrices}

Given a symmetric matrix $A = A^T\in\reals^{n\times n}$,
all the eigenvalues are real and we can choose $n$ real orthonormal eigenvectors,
\ie,
we can find $n$ eigenvectors $v_1, \ldots, v_n\in\reals^n$
associated with $n$ eigenvectors, $\lambda_1, \ldots, \lambda_n \in \reals$
such that
\begin{equation}
    \|v_i\| = 1
\end{equation}
for $i=1,\ldots,n$
and
\begin{equation}
    v_i^T v_j = 0
\end{equation}
for $1\leq i\neq j\leq n$.
Thus, all symmetric matrices are diagonalizable.

Now (\ref{eq:2}) becomes
\begin{equation}
\label{eq:sym-eigen-decomp}
A = V \Lambda V^T \Leftrightarrow V^T A V = \Lambda
\end{equation}
since
\begin{equation}
V^T V = I_n
\end{equation}
where $I_n\in\reals^{n\times n}$ is the indentity matrix.
We can rewrite (\ref{eq:sym-eigen-decomp}) as
\begin{equation}
\label{eq:sym-eigen-decomp-1}
A =
\begin{my-matrix}{ccc} v_1 & \cdots & v_n \end{my-matrix}
\diag(\lambda_1,\ldots,\lambda_n)
\begin{my-matrix}{c} v_1^T \\ \vdots \\ v_n^T \end{my-matrix}
= \sum_{i=1}^n \lambda_i v_i v_i^T.
\end{equation}

\subsection{Positive definiteness}

\begin{itemize}

\item
A symmetric matrix $A=A^T\in\reals^{n\times n}$ is called positive semidefinite if for all $x\in\reals^n$,
\begin{equation}
x^T A x \geq 0.
\end{equation}

\item
A symmetric matrix $A=A^T\in\reals^{n\times n}$ is called positive definite if for all nonzero $x\in\reals^n$,
\begin{equation}
x^T A x > 0.
\end{equation}

\item The set of all the $n$-by-$n$ positive semidefinite matrices is (sometimes) denoted by $\possemidefset{n}$,
\ie,
\begin{equation}
\possemidefset{n} = \set{A=A^T\in\reals^{n\times n}}{x^T A x \geq 0 \mbox{ for all } x \in \reals^n}.
\end{equation}

\item The set of all the $n$-by-$n$ positive definite matrices is (sometimes) denoted by $\posdefset{n}$,
\ie,
\begin{equation}
\posdefset{n} = \set{A=A^T\in\reals^{n\times n}}{x^T A x > 0 \mbox{ for all nonzero } x \in \reals^n}.
\end{equation}

\item $A=A^T\in\reals^{n\times n}$ is positive semidefinite if and only if all the eigenvalues of $A$ are nonnegative.

\item $A=A^T\in\reals^{n\times n}$ is positive definite if and only if all the eigenvalues of $A$ are positive.

\begin{proof}
For symmetric $A=A^T$, there exist orthgonal $V\in\reals^{n\times n}$ and diagonal $\Lambda\in\reals^{n\times n}$
such that
\[
A = V \Lambda V^T = \sum_{i=1}^n \lambda_i v_i v_i^T,
\]
thus for any $x\in\reals^n$,
\[
x^T A x = x^T \left(\sum_{i=1}^n \lambda_i v_i v_i^T \right) x
= \sum_{i=1}^n \lambda_i x^T v_i v_i^T x
= \sum_{i=1}^n \lambda_i (v_i^T x)^2.
\]
Therefore if all $\lambda_i$ are nonnegative, $x^T A x\geq0$ for any $x\in\reals^n$, hence $A\in\possemidefset{n}$.
Now assume $A\in\possemidefset{n}$, but $\lambda_j < 0$ for some $j\in\{1,\ldots,n\}$.
Then
\begin{equation}
v_j^T A v_j
= \sum_{i=1}^n \lambda_i (v_i^T v_j)^2
= \sum_{i=1}^n \lambda_i \delta_{i,j}
= \lambda_j < 0
\end{equation}
since $v_1$, \ldots, $v_n$ are orthonormal
where
$\delta_{i,j}$ is the \href{https://en.wikipedia.org/wiki/Kronecker_delta}{Kronecker delta function},
hence $A\not \in \possemidefset{n}$.
Therefore if $A\in\possemidefset{n}$, all $\lambda_i$ are nonnegative.

Therefore $A\in\possemidefset{n}$ if and only if all $\lambda_i$ are nonnegative.

Now assume that all $\label_i$ are positive.
Then for all nonzero $x\in\reals^n$,
there exists $i\in\{1,\ldots,n\}$ such tat $v_i^Tx$
since if $v_i^Tx=0$ for all $i$, then
$V^T x = 0$, hence $x=0$ since $V^T$ is nonnsigular.
Therefore
\begin{equation}
x^T A x = \sum_{i=1}^n \lambda_i (v_i^T x)^2
\geq \lambda_j (v_j^T x)^2 > 0.
\end{equation}
Thus, $A\in\posdefset{n}$.

Now assume that $A\in\posdefset{n}$.
If $\lambda_j \leq 0$ for some $j\in\{1,\ldots,n\}$,
then
\begin{equation}
v_j^T A v_j
= \sum_{i=1}^n \lambda_i \delta_{i,j}
= \lambda_j \leq 0,
\end{equation}
hence $A\not\in\posdefset{n}$. Therefore if $A\in\posdefset{n}$, all $\lambda_i$ are positive.

Therefore $A\in\posdefset{n}$ if and only if all $\lambda_i$ are positive.

\end{proof}



\end{itemize}

\newpage
\section{Optimization}

\subsection{Mathematical optimization problem}

A mathematical optimization problem can be expressed as
\begin{equation}
\label{eq:opt-prob}
\begin{array}{ll}
\mbox{minimize} & f_0(x)
\\
\mbox{subject to} & f_i(x) \leq 0 \mbox{ for } i = 1, \ldots, m
\\
& h_i(x) = 0 \mbox{ for } i = 1, \ldots, p
\end{array}
\end{equation}
where
$x\in\reals^n$ is the optimization variable,
$f_0:\reals^n\to\reals$ is the objective function,
$f_i:\reals^n\to\reals$ for $i=1,\ldots,n$ are the inequality constraint functions,
and
$h_i:\reals^n\to\reals$ for $i=1,\ldots,p$ are the equality constraint functions.

The conditions, $f_i(x) \leq 0$ for $ i = 1, \ldots, m$, are called inequality constraints
and the conditions, $ h_i(x) = 0 $ for $ i = 1, \ldots, p$ are called equation constraints.

Note that this formulation covers pretty much every single-objective optimization problem.
For example,
consider the following optimization problem.

\begin{equation}
\begin{array}{ll}
\mbox{maximize} & f(x_1,x_2)
\\
\mbox{subject to} & x_1 \geq x_2
\\
& x_1 + x_2 = 2
\end{array}
\end{equation}
This problem can be cast into an equivalent problem as follows.
\begin{equation}
\begin{array}{ll}
\mbox{minimize} & -f(x_1,x_2)
\\
\mbox{subject to} & - x_1 + x_2 \leq 0
\\
& x_1 + x_2 - 2 = 0
\end{array}
\end{equation}


The feasible set for (\ref{eq:opt-prob}) is defined by the set of $x\in\reals^n$ which satisfies all the contraints.
Also, the optimal value for (\ref{eq:opt-prob}) is the infimum of $f_0(x)$ while $x$ is in the feasible set.
When the infimum is achievable, we define the optimal solution set as the set of all feasible $x$ achieving
the infimum value.
These are defined in mathematically rigorous terms below.

\begin{itemize}

\item
The feasible set for (\ref{eq:opt-prob}) is defined by
\begin{equation}
\feasibleset
=
\set{x\in \mathcal{D}}
{ f_i(x)\leq 0 \mbox{ for } i =0, \ldots, m,\ h_j(x) = 0 \mbox{ for } j = 1,\ldots,p}
\subseteq \reals^n
\end{equation}
where
\begin{equation}
\mathcal{D} = \left( \bigcap_{0\leq i\leq m} \dom f_i \right) \cap \left( \bigcap_{1\leq i\leq p} \dom h_i \right).
\end{equation}

\item
The optimal value for (\ref{eq:opt-prob}) is defined by
\begin{equation}
p^\ast = \inf_{x\in\feasibleset} f_0(x)
\end{equation}

We use the convetions that $p^\ast = -\infty$ if $f_0(x)$ is unbounded below for $x\in \feasibleset$
and that $p^\ast = \infty$ if $\feasibleset = \emptyset$.

\item
The optimal solution set for (\ref{eq:opt-prob}) is defined by
\begin{equation}
\optsolset = \set{x\in\feasibleset}{f_0(x) = p^\ast}.
\end{equation}


\end{itemize}


\subsection{Convex optimization problem}

A mathematical optimization problem is called a convex optimization problem
if the objective function and all the inequality constraint functions are convex functions
and all the equality constraint functions are affine functions.

Hence, a convex optimization problem can be expressed as
\begin{equation}
\label{eq:cvx-opt-prob}
\begin{array}{ll}
\mbox{minimize} & f_0(x)
\\
\mbox{subject to} & f_i(x) \leq 0 \mbox{ for } i = 1, \ldots, m
\\
& A x = b
\end{array}
\end{equation}
where
$x\in\reals^n$ is the optimization variable,
$f_i:\reals^n\to\reals$ for $i=0,\ldots,n$ are convex functions,
$h_i:\reals^n\to\reals$ for $i=1,\ldots,p$ are the equality constraint functions,
$A \in \reals^{p\times n}$, and $b\in\reals^p$.

A function, $f:\reals^n \to \reals$, is called a convex function if
$\dom f \subseteq \reals^n$ is a convex set
and
for all $x, y\in\dom f$
and all $0\leq \lambda \leq 1$,
\begin{equation}
    f( \lambda x + (1-\lambda) y) \leq
    \lambda f(x) + (1-\lambda) f(y)
\end{equation}
where $\dom f \subseteq \reals^n$ denotes the domain of $f$.

A convex optimization enjoys a number of nice theoretical and practical properties.

\begin{itemize}
\item A local minimum of a convex optimization problem is a global minimum,
\ie,
if for some $R>0$ and $x_0\in\feasibleset$, $\|x-x_0\|<R$ and $x\in\feasibleset$ imply $f_0(x_0) \leq f_0(x)$,
then $f_0(x_0) \leq f_0(x)$ for all $x\in\feasibleset$.
\begin{proof}
Assume that $x_0\in\feasibleset$ is a local minimum, \ie,
for some $R>0$, $\|x-x_0\|<R$ and $x\in\feasibleset$ imply $f_0(x_0) \leq f_0(x)$.

Now assume that $x_0$ is not a global minimum, \ie, there exists $y\in\feasibleset$
such that $y\neq x_0$ and $f_0(y) < f_0(x_0)$.
Then for $z = \lambda y + (1-\lambda) x_0$ with $\lambda = \min\{ R/\|y-x_0\|, 1\}/2$,
the convexity of $f_0$ implies
\begin{equation}
\label{eq:4}
f_0(z) \leq \lambda f_0(y) + (1-\lambda) f_0(x_0)
\end{equation}
since $0 < \lambda \leq 1/2 < 1$.
Furthermore
\begin{equation}
\|z - x_0\| = \lambda \|y-x_0\| \leq R/2,
\end{equation}
hence $f_0(z) \geq f_0(x_0)$, which together with (\ref{eq:4}) implies
\begin{equation}
f_0(x_0) \leq f_0(z)
\leq \lambda f_0(y) + (1-\lambda) f_0(x_0)
< \lambda f_0(x_0) + (1-\lambda) f_0(x_0)
= f_0(x_0),
\end{equation}
which is a contradiction.
Therefore there is no $y\in\feasibleset$ such that $y\neq x_0$ and $f_0(y) < f_0(x_0)$.
Therefore $x_0$ is a global minimum.
\end{proof}



\item For a unconstrained problem, \ie, the problem (\ref{eq:cvx-opt-prob}) with $m=p=0$, with differential objective function,
$x\in\dom f_0$ is an optimal solution if and only if $\grad f_0(x)= 0 \in \reals^n$.

\begin{proof}
The Taylor theorem implies that for any $x,y\in\dom f_0$,
\begin{equation}
\label{eq:second-order-taylor}
f_0(y) = f(x) + \grad f_0(x) ^T (y-x) + \frac{1}{2} (y-x)^T \grad^2 f_0(z) (y-x)
\end{equation}
for some $z$ on the line segment having $x$ and $y$ as its end points,
\ie, $z = \alpha x + (1-\alpha) y$ for some $0\leq \alpha \leq 1$.
Since $\grad^2 f(x) \succeq0$ for any $z \in \dom f_0$, we have
\begin{equation}
f_0(y) \geq f_0(x) + \grad f_0(x) ^T (y-x)
\end{equation}

Thus, if for some $x_0 \in \reals^n$, $\grad f_0(x_0) = 0$, for any $x\in\dom f_0$,
\begin{equation}
f_0(x) \geq f_0(x_0) + \grad f_0(x_0) ^T (x-x_0) = f_0(x_0),
\end{equation}

hence $x_0$ is an optimal solution.
Now assume that $x_0$ is an optimal solution, but $\grad f_0(x_0) \neq 0$.
Then for any $k>0$, if we let $x=x_0$ and $y = x_0 - k \grad f_0(x_0) $,
(\ref{eq:second-order-taylor}) becomes
\begin{eqnarray*}
\lefteqn{
f_0(y) = f(x_0) + \grad f_0(x_0) ^T (-k \grad f_0(x_0)) + \frac{k^2}{2} \grad f_0(x_0) ^T \grad^2 f_0(z) \grad f_0(x_0)
}
\\
&=&
f(x_0) - k \|\grad f_0(x_0)\|^2 + \frac{k^2}{2} \grad f_0(x_0) ^T \grad^2 f_0(z) \grad f_0(x_0)
\end{eqnarray*}
for all $y = x_0 - k \grad f_0(x_0) \in \dom f_0$.

Since for $k< 2 \|\grad f_0(x_0)\|^2 / \grad f_0(x_0) ^T \grad^2 f_0(z) \grad f_0(x_0)$,
$-k \|\grad f_0(x_0)\|^2 + \frac{k^2}{2} \grad f_0(x_0) ^T \grad^2 f_0(z) \grad f_0(x_0) < 0$,
thus
$f_0(y) < f(x_0)$,
hence the constradiction.
Therefore, if $x_0$ is an optimal solution for the unconstrained problem, $\grad f_0(x_0) = 0$.

\end{proof}
\end{itemize}


\newpage
\section{Simple portfolio optimization problem}

\subsection{Problem formulation}

Suppose that we have $n$ assets to invest on
and that the return of each asset per unit invest is modeled by random variables $R_i$ for $i=1,\ldots,n$.
Then we want to decide the amount of investment on each asset, $x_i\in\reals$ for $i=1,\ldots,n$,
so that it optimizes the overall investment (in certain senses).

For formulization, we use the following definitions.

\begin{itemize}
\item Define a vector random variable $R\in\reals^n$ such that
\begin{equation}
R = \begin{my-matrix}{c}
R_1
\\
\vdots
\\
R_n
\end{my-matrix}
\in\reals^n.
\end{equation}

\item Let $r\in\reals^n$ be the expected value of $R$,
\ie,
\begin{equation}
r
= \Expect(R)
= \begin{my-matrix}{c}
\Expect(R_1)
\\
\vdots
\\
\Expect(R_n)
\end{my-matrix}
= \begin{my-matrix}{c}
r_1
\\
\vdots
\\
r_n
\end{my-matrix}
\in\reals^n.
\end{equation}

\item Define a vector $x\in\reals$ which is an aggregate of the investments:
\begin{equation}
x = \begin{my-matrix}{c}
x_1
\\
\vdots
\\
x_n
\end{my-matrix}
\in\reals^n.
\end{equation}

\item Define a feasible set $\mathcal{X}\subseteq \reals^n$ for $x$.
For example, if we have a limit on the total investment,
\begin{equation}
\label{eq:cnst-on-cost}
\mathcal{X} = \set{x\in\reals^n}{\sum_{i=1}^n c_i x_i \leq c_\mathrm{max}},
\end{equation}
or if we have the minimum and maximum amount to invest for each asset,
we'd have
\begin{equation}
\label{eq:cnst-on-each-amount}
\mathcal{X} = \set{x\in\reals^n}{d_\mathrm{min} \leq x_i \leq d_\mathrm{max} \mbox{ for } i=1,\ldots,n}.
\end{equation}
Generally, we'd prefer $\mathcal{X}$ to be a convex set, \ie,
for any $x,y\in\mathcal{X}$ and $0\leq \lambda \leq 1$,
\begin{equation}
\lambda x + (1-\lambda) y \in \mathcal{X}.
\end{equation}

\end{itemize}

\subsubsection{A portfolio optimization problem}

A portfolio optimization problem can be formulized by
\begin{equation}
\label{eq:opt-port-prob}
\begin{array}{ll}
\mbox{maximize} & f(x) = \Expect(Z)
\\
\mbox{minimize} & g(x) = \Var(Z)
\\
\mbox{subject to} & x \in \mathcal{X}
\end{array}
\end{equation}
where the optimization variable is $x\in\reals^n$
and
\begin{equation}
Z = \sum_{i=1}^n x_i R_i = x^T R
\end{equation}
where $\Expect(\cdot)$ and $\Var(\cdot)$ refer to the expected value and the variance operators respectively.

This problem formulation tries to \emph{maximize the expected return}
while \emph{minimizing the variance or uncertainty or risk}, which generally makes sense.

(\ref{eq:cnst-on-cost})
(\ref{eq:cnst-on-each-amount})


Note that
\begin{equation}
\Expect(Z)
= \Expect(x^T R)
= \Expect \left(\sum_{i=1}^n x_i R_i \right)
= \sum_{i=1}^n x_i \Expect(R_i)
= \sum_{i=1}^n x_i r_i
= r^T x
\end{equation}
and
\begin{eqnarray*}
\lefteqn{
\Var(Z) = \Expect(Z-\Expect(Z))^2 = \Expect \left(x^TR-x^Tr \right)^2
}
\\
&=&
\Expect \left(x^T(R-r) \right)^2
= \Expect \left(x^T(R-r)(R-r)^T x \right)
\\
&=&
x^T \Expect(R-r)(R-r)^T x
=
x^T \covmat{R} x
\end{eqnarray*}
where $\covmat{R} = \Expect(R-r)(R-r)^T$ is the \href{https://en.wikipedia.org/wiki/Covariance_matrix}{covariance matrix} of $R$.
Note that $\covmat{R}\in\possemidefset{n}$
since for any $y\in\reals^n$,
\begin{equation}
y^T \covmat{R} y = y^T \Expect(R-r)(R-r)^T y = \Expect(x^T(R-r))^2 \geq 0.
\end{equation}



Thus, (\ref{eq:opt-port-prob}) can be rewritten as
\begin{equation}
\label{eq:opt-port-prob-vec}
\begin{array}{ll}
\mbox{maximize} & f(x) = r^T x
\\
\mbox{minimize} & g(x) = x^T \covmat{R} x
\\
\mbox{subject to} & x \in \mathcal{X}.
\end{array}
\end{equation}





\end{document}
