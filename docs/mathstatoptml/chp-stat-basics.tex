
\section{Probability space and random variables}
\subsection{Correlation coefficients}

The correlation coefficients of two random variables, $X$ and $Y$, is defined by

\begin{equation}
\rho_{X,Y} = \frac{\Expect (X-\mu_X)(Y-\mu_Y)} {\sqrt{\Expect (X-\mu_X)^2 \Expect(Y-\mu_Y)^2}}
\end{equation}


\section{Transformation of a random variable via a function}

\subsection{Scale random variable}

Assume two random variables, $X\in\reals$ and $Y\in\reals$, which are related by
a function $g:\reals \to \reals \in C^{1}$
such that
\begin{equation}
\label{eq:g8cx}
Y = g(X).
\end{equation}

Now let's derive an equation for the probability density function (PDF) of $Y$
given the PDF of $X$, $f_X:\reals\to\preals$.

The definition of cumulative distribution function (CDF) of $Y$ implies that
\begin{equation}
\label{eq:gnpz}
    F_Y(y) = \Prob\{Y \leq y\} = \Prob\{g(X) \leq y\}
\end{equation}
for any $y\in\reals$.

Now if we assume that $g$ is a strictly increasing function, it has its inverse function $g^{-1}: g(\reals) \to \reals$
and
(\ref{eq:gnpz}) becomes
\[
    F_Y(y) = \Prob\{g(X) \leq y\}
    = \Prob\{X \leq g^{-1}(y)\}
    = F_X(g^{-1}(y))
\]
for any $y\in g(\reals)$.
Thus, we can differentiate both sides to have
\begin{equation}
\label{eq:gnpz-1}
    f_Y(y) = \frac{d}{dy} F_Y(y)
    = \frac{d}{dy} F_X(g^{-1}(y))
    = f_X(g^{-1}(y)) \frac{d}{dy} g^{-1}(y)
    = \frac{1}{g'(g^{-1}(y))} f_X(g^{-1}(y)),
\end{equation}
since (\ref{theorem:chain-rule}) implies that
\begin{equation}
1
= \frac{d}{dx} g(g^{-1}(x))
= g'(g^{-1}(x)) \frac{d}{dx} g^{-1}(x),
\end{equation}
\ie, the derivative of the inverse function
is the inverse of the derivative of the original function.

Now if we assume that $g$ is a strictly decreasing function, we have
\[
    F_Y(y) = \Prob\{g(X) \leq y\}
    = \Prob\{x \geq g^{-1}(y)\}
    = 1 - F_X(g^{-1}(y))  + \Prob\{x = g^{-1}(y)\},
\]
and
\begin{equation}
\label{eq:gnpz-2}
    f_Y(y) = \frac{d}{dy} F_Y(y)
    = - \frac{d}{dy} F_X(g^{-1}(y))
    = - f_X(g^{-1}(y)) \frac{d}{dy} g^{-1}(y)
    = - \frac{1}{g'(g^{-1}(y))} f_X(g^{-1}(y)).
\end{equation}
Since $g'(y)>0$ for a strictly increasing function,
and $g'(y)<0$ for a strictly decreasing function,
(\ref{eq:gnpz-1}) and (\ref{eq:gnpz-2}) imply
\begin{equation}
\label{eq:pdf-rv-transform-1}
    f_Y(y) = \frac{1}{|g'(g^{-1}(y))|} f_X(g^{-1}(y))
\end{equation}
for both cases.

Now consider a general function, $g$, \ie, not necessarily a monotonic function.
Suppose that $y\in g(\reals)$.
Then for every $x\in\reals$ such that $f(x) = y$,
if $f'(x)\neq0$, then $f(x)$ is locally strictly monotonic,
\ie, there exists $\delta>0$ such that $f(x)$ is strictly monotonic for $x\in(x-\delta, x + \delta)$,
hence (\ref{eq:pdf-rv-transform-1}) holds for such $x$.

The probability around $y$ is a summation of the probabilities around such points,
\ie, the probabilities around all $x$ such that $f(x) =y$ and $f'(x)\neq0$.
Therefore, we have
\begin{equation}
\label{eq:pdf-rv-transform}
    f_Y(y) = \sum_{x:g(x)=y}\frac{1}{|g'(x)|} f_X(x).
\end{equation}

There is another way to derive the same equation (in a less strict way)
which helps get more insight.
Let's again suppose that $g$ is a strictly increasing function.
Then consider the probability that $X$ lies in $(x, x + \Delta x)$.
The probability should be the same as $Y$ lies in $(y, y + \Delta y)$
where $y=g(x)$ and $\Delta y = g(x+\Delta x) - g(x)$,
\ie,
\begin{eqnarray*}
\lefteqn{
f_X(x) \Delta x
\approx
\int_{x}^{x + \Delta x} f_X(x) \, dx
=
\Prob\{x\leq X\leq x + \Delta x\}
}
\\
&=&
\Prob\{y\leq Y\leq y + \Delta y\}
=
\int_{y}^{y + \Delta y} f_Y(y) \, dy
\approx
f_Y(y) \Delta y.
\end{eqnarray*}
The approximation becomes the equality when $\Delta x$ goes to $0$.
Therefore we have
\begin{equation}
f_Y(y)
= \lim_{\Delta x \to 0}\frac{\Delta x}{\Delta y} f_X(x)
= \frac{1}{\lim_{\Delta x \to 0} \frac{g(x+\Delta x)-g(x)}{\Delta x}} f_X(x)
= \frac{1}{g'(x)} f_X(x),
\end{equation}
which is equivalent to (\ref{eq:gnpz-1}).
Following the very same argument as before will lead to (\ref{eq:pdf-rv-transform}),
\ie, applying the same method to strictly decreasing case, \etc\



\subsection{Multivariate random variable}

%Assume two random variables, $X\in\reals^n$ and $Y\in\reals^n$, which are related by
%a function $g:\reals^n \to \reals^n$
%such that
%\begin{equation}
%\label{eq:wnux-1}
%Y = g(X).
%\end{equation}
%Assume that $g$ is differentiable everywhere in $\dom g$,
%\ie, the Jacobian matrix defined in (\ref{eq:jacobian-matrix}) exists for all $x \in \dom g$.
%
%XXX
%
%Now we suppose that $g$ is a strictly increasing and invertible function.
%Then the definition of cumulative distribution function (CDF) of $Y$ implies that
%\begin{equation}
%\label{eq:wnux-2}
%    F_Y(y) = \Prob\{Y \preceq y\} = \Prob\{g(X) \preceq y\}
%\end{equation}
%for any $y\in\reals^n$ where $\preceq$ is the componentwise inequality.
%Since $g$ is an invertible and stricly increasing function, the inverse function $g^{-1}: g(\reals^n) \to \reals^n$
%exists and
%(\ref{eq:wnux-2}) becomes
%\[
%    F_Y(y) = \Prob\{g(X) \preceq y\}
%    = \Prob\{X \preceq g^{-1}(y)\}
%    = F_X(g^{-1}(y))
%\]
%for any $y\in g(\reals^n)$.
%Thus, we can differentiate both sides to have
%\begin{equation}
%\label{eq:gnpz-1}
%    f_Y(y) = \frac{d}{dy} F_Y(y)
%    = \frac{d}{dy} F_X(g^{-1}(y))
%    = f_X(g^{-1}(y)) \frac{d}{dy} g^{-1}(y)
%    = \frac{1}{g'(g^{-1}(y))} f_X(g^{-1}(y)),
%\end{equation}
%since (\ref{theorem:chain-rule}) implies that
%\begin{equation}
%1
%= \frac{d}{dx} g(g^{-1}(x))
%= g'(g^{-1}(x)) \frac{d}{dx} g^{-1}(x),
%\end{equation}
%\ie, the derivative of the inverse function
%is the inverse of the derivative of the original function.
%
%Now if we assume that $g$ is a strictly decreasing function, we have
%\[
%    F_Y(y) = \Prob\{g(X) \leq y\}
%    = \Prob\{x \geq g^{-1}(y)\}
%    = 1 - F_X(g^{-1}(y))  + \Prob\{x = g^{-1}(y)\},
%\]
%and
%\begin{equation}
%\label{eq:gnpz-2}
%    f_Y(y) = \frac{d}{dy} F_Y(y)
%    = - \frac{d}{dy} F_X(g^{-1}(y))
%    = - f_X(g^{-1}(y)) \frac{d}{dy} g^{-1}(y)
%    = - \frac{1}{g'(g^{-1}(y))} f_X(g^{-1}(y)).
%\end{equation}
%Since $g'(y)>0$ for a strictly increasing function,
%and $g'(y)<0$ for a strictly decreasing function,
%(\ref{eq:gnpz-1}) and (\ref{eq:gnpz-2}) imply
%\begin{equation}
%\label{eq:pdf-rv-transform-1}
%    f_Y(y) = \frac{1}{|g'(g^{-1}(y))|} f_X(g^{-1}(y))
%\end{equation}
%for both cases.
%
%Now consider a general function, $g$, \ie, not necessarily a monotonic function.
%Suppose that $y\in g(\reals)$.
%Then for every $x\in\reals$ such that $f(x) = y$,
%if $f'(x)\neq0$, then $f(x)$ is locally strictly monotonic,
%\ie, there exists $\delta>0$ such that $f(x)$ is strictly monotonic for $x\in(x-\delta, x + \delta)$,
%hence (\ref{eq:pdf-rv-transform-1}) holds for such $x$.
%
%The probability around $y$ is a summation of the probabilities around such points,
%\ie, the probabilities around all $x$ such that $f(x) =y$ and $f'(x)\neq0$.
%Therefore, we have
%\begin{equation}
%\label{eq:pdf-rv-transform}
%    f_Y(y) = \sum_{x:g(x)=y}\frac{1}{|g'(x)|} f_X(x).
%\end{equation}
%
%There is another way to derive the same equation (in a less strict way)
%which helps get more insight.
%Let's again suppose that $g$ is a strictly increasing function.
%Then consider the probability that $X$ lies in $(x, x + \Delta x)$.
%The probability should be the same as $Y$ lies in $(y, y + \Delta y)$
%where $y=g(x)$ and $\Delta y = g(x+\Delta x) - g(x)$,
%\ie,
%\begin{eqnarray*}
%\lefteqn{
%f_X(x) \Delta x
%\approx
%\int_{x}^{x + \Delta x} f_X(x) \, dx
%=
%\Prob\{x\leq X\leq x + \Delta x\}
%}
%\\
%&=&
%\Prob\{y\leq Y\leq y + \Delta y\}
%=
%\int_{y}^{y + \Delta y} f_Y(y) \, dy
%\approx
%f_Y(y) \Delta y.
%\end{eqnarray*}
%The approximation becomes the equality when $\Delta x$ goes to $0$.
%Therefore we have
%\begin{equation}
%f_Y(y)
%= \lim_{\Delta x \to 0}\frac{\Delta x}{\Delta y} f_X(x)
%= \frac{1}{\lim_{\Delta x \to 0} \frac{g(x+\Delta x)-g(x)}{\Delta x}} f_X(x)
%= \frac{1}{g'(x)} f_X(x),
%\end{equation}
%which is equivalent to (\ref{eq:gnpz-1}).
%Following the very same argument as before will lead to (\ref{eq:pdf-rv-transform}),
%\ie, applying the same method to strictly decreasing case, \etc\


\subsection{Data Examples}

Suppose that we have $n$ random variables, $X_1$, \ldots, $X_n$
and they are independent and identically distributed Gaussian, $\mathcal{N}(0,1)$.
Then assume that a random variable, $Y$, is the sum of the $X_i$'s,
\ie,
\begin{equation}
Y = \sum_{i=1}^n X_i = X_1 + \cdots + X_n
\end{equation}

Then the covariance of $X_i$ and $Y$ for each $i$ is
\begin{equation}
\mathbf{Cov}(X_i,Y) = \Expect (X_i - \Expect{X_i})( Y - \Expect Y) = \Expect\left( \sum_{j=1}^n X_iX_j\right) = 1
\end{equation}
and
the variance of $Y$ is
\begin{equation}
\mathbf{Var}(Y) = \Expect (Y - \Expect Y)^2 = \Expect\left( \sum_{j=1}^n X_iX_j\right)^2
= \sum_{i=1}^n \Expect X_i^2 = n.
\end{equation}

Hence, the correlation coefficient of $X_i$ and $Y$ for each $i$ is
\begin{equation}
\rho_{X_i,Y} = \mathbf{Cov}(X_i,Y) / \sqrt{\mathbf{Var}X_i \mathbf{Var} Y} = 1 / \sqrt(n).
\end{equation}

\emph{Therefore $Y$ has clear relation with $X_i$'s, but each correlation coefficient can be arbitrarily small as $n$ grows!}


\section{Empirical Cumulative Distribution Function}

Suppose that we have $n$ (scalar) data points, $x_1, \ldots, x_n \in\reals$.
We assume that they are from some probability distribution of a random variable $X$.
The empirical cumulative distribution function (ECDF) is one of the non-parameterized estimation methods
to estimate the distribution of the original random variable, $X$.

The ECDF is defined by
\begin{equation}
\hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n u(x-x_i)
\end{equation}
where the step function $u:\reals\to\reals$ is defined by
\begin{equation}
u(x) = \left\{\begin{array}{ll}
1 & \mbox{if } x \geq 0
\\
0 & \mbox{otherwise.}
\end{array}\right.
\end{equation}

Under some mild conditions, this ECDF converges to the cumulative distribution function (CDF) of the original random variable,
\ie,
\begin{equation}
\lim_{n\to\infty} F_n(x) = F_X(x)
\end{equation}
where $F_X:\reals\to\reals$ is the true cumulative distribution function,
\ie,
\begin{equation}
F_X(x) = \Prob\{X\leq x\}.
\end{equation}

\subsection{Mixture distribution of ECDF}

Suppose that we have $m$ set of data $\{x_{ji}\}_{i=1}^{n_j}$
where $j=1,\ldots, m$ and $n_j$ refers to the size of the $j$th data set.

If we assume that the $j$th data set is drawn from a random variable $X_j$,
the $j$th ECDF is an approximate of the $j$th true CDF,
\ie,
\begin{equation}
\hat{F}_j(x) = \frac{1}{n_j} \sum_{i=1}^{n_j} u(x-x_{ji})
\sim F_{X_j}(x).
\end{equation}

Now let us figure out what is the relationship between the ECDF formed by all these data points
and the $m$ original CDFs.
Note that there are $\sum_{j=1}^m n_j$ data points in total.

It turns out that the ECDF formed by all data points is an approximate for the mixture distribution
of $X_1$, \ldots, $X_m$
where the $j$th mixture probability is $\pi_j = n_j / \sum_{k=1}^m n_k$.
Note that $\sum_{j=1}^m \pi_j = 1$.

To be more precise, let us defined a random variable $\tilde{X}$
the probability density function (PDF) of which is defined by
\begin{equation}
\label{eq:gyusj}
f_{\tilde{X}}(x) = \sum_{j=1}^m \pi_j f_{X_j}(x).
\end{equation}
Then the CDF of $\tilde{X}$ is
\begin{equation}
F_{\tilde{X}}(x)
= \int_{-\infty}^x f_{\tilde{X}}(\tilde{x}) d\tilde{x}
= \sum_{j=1}^m \pi_j \int_{-\infty}^x f_{X_j}(\tilde{x}) d\tilde{x}
= \sum_{j=1}^m \pi_j F_{X_j}(x).
\end{equation}

Now the ECDF obtained from \emph{all} $\sum_{j=1}^m n_j$ data points is (by definition)
\begin{eqnarray*}
\hat{F}(x) &=& \frac{1}{\sum_{j=1}^m n_j} \sum_{j=1}^m \sum_{i=1}^{n_j} u(x-x_{ji})
= \frac{1}{\sum_{j=1}^m n_j} \sum_{j=1}^m \frac{n_j}{n_j} \sum_{i=1}^{n_j} u(x-x_{ji})
\\
&=&
\sum_{j=1}^m \frac{n_j}{\sum_{j=1}^m n_j} \left( \frac{1}{n_j} \sum_{i=1}^{n_j} u(x-x_{ji}) \right)
\\
&=&
\sum_{j=1}^m \pi_j \hat{F}_j(x).
\end{eqnarray*}

Since $\lim_{n_j\to\infty} \hat{F}_j(x) = F_{X_j}(x)$ for each $j$,
\begin{equation}
\lim_{n_1, \ldots, n_j \to\infty} \hat{F}(x) = \sum_{j=1}^m \pi_j F_{X_j}(x) = \hat{F}_{\tilde{X}}(x),
\end{equation}
\ie,
the ECDF converges to the CDF of $\tilde{X}$.

Therefore the ECDF formed by all the data points $\{x_{ji}\}_{i=1}^{n_j}$
is the ECDF for the mixture distribution defined by (\ref{eq:gyusj}).


\subsection{ECDF of mixture distribution}

We discussed what distribution a mixture of data points from different distributions represent.
Here we try to do (kind of) converse,
\ie,
given arbitrary mixture probabilities, $\pi_j$ ($j=1,\ldots,m$),
form ECDF approximating the following mixture distribution.
\begin{equation}
\label{eq:fydhf}
\sum_{j=1}^m \pi_j f_{X_j}(x).
\end{equation}

It can be proved that the following (modified) ECDF approximates the mixture distribution in (\ref{eq:fydhf}).
\begin{equation}
\hat{F}(x) = \sum_{j=1}^m \frac{\pi_j}{n_j} \sum_{i=1}^{n_j} u(x- x_{ji}).
\end{equation}


\section{Various distributions}

\subsection{Gaussian distribution}

The Gaussian (or normal) distribution is a type of continuous probability distribution for a real-valued random variable
or real-valued random variables.
The PDF of (vector) Gaussian random variable $X\in\reals^n$ with mean $\mu\in\reals^n$ and covariance matrix $\Sigma\in\posdefset{n}$ is
defined by $f_X:\reals^n\to\preals$ where
\begin{equation}
\label{eq:pdf:gaussian}
f_X(x) = \frac{1}{(2\pi)^{n/2}\det(\Sigma)^{1/2}} \exp\left(
-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)
\right).
\end{equation}
We use the notation $X\sim \mathcal{N}(\mu, \Sigma)$ to denote that $X$ is a Gaussian random variable with
with mean $\mu\in\reals^n$ and covariance matrix $\Sigma\in\posdefset{n}$.

If $n=1$, this reduces to a scalar Gaussian random variable the PDF of which is given by
\begin{equation}
f_X(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left( - (x-\mu)^2 /2\sigma^2 \right)
\end{equation}
where $\mu\in\reals$ is the mean and $\sigma^2$ is the variance.

In a statistical inference or machine learning context, we are sometimes interested in log-likelihood.
To evaluate the log-likelihood, we need to take logarithm on the PDF.
For Gaussian random variable,
this quantity is given by
\begin{equation}
\log f_X(x) = -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) - \frac{n}{2} \log(2\pi) - \frac{1}{2} \log \det(\Sigma).
\end{equation}







\subsection{Log-normal distribution}

 We say $Y$ is log-normally distributed, if, for $X\sim\mathcal{N}(\mu_X,\sigma_X^2)$,
 \begin{equation}
 Y = \exp(X).
 \end{equation}

Then (\ref{eq:pdf-transform}) implies that
\begin{eqnarray}
f_Y(y) &=& \frac{1}{\exp(\log(y))} \cdot \frac{1}{\sqrt{2\pi} \sigma_X}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right)
\nonumber
\\
&=& \frac{1}{\sqrt{2\pi} \sigma_X y}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right).
\label{eq:log-normal-pdf}
\end{eqnarray}


\subsubsection{Some statistics}

The definition of the expected value implies
\begin{eqnarray*}
\lefteqn{
\Expect Y = \int_{0}^{\infty} y f_Y(y) \, dy
= \int_{0}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right) dy
}
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x-\mu_X)^2}{2\sigma_X^2}\right) \exp(x) \, dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x-\mu_X)^2}{2\sigma_X^2}+x\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{x^2 - 2(\mu_X +\sigma_X^2)x + \mu_X^2}{2\sigma_X^2}\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +\sigma_X^2))^2 + \mu_X^2 -(\mu_X+\sigma_X^2)^2}{2\sigma_X^2}\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +\sigma_X^2))^2 - 2\mu_X\sigma_X^2 - \sigma_X^4}{2\sigma_X^2}\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +\sigma_X^2))^2 - \sigma_X^2(2\mu_X+ \sigma_X^2)}{2\sigma_X^2}\right) dx
\\
&=&
\exp\left(\frac{2\mu_X+ \sigma_X^2}{2}\right)
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +\sigma_X^2))^2 }{2\sigma_X^2}\right) dx
\\
&=&
\exp\left(\frac{2\mu_X+ \sigma_X^2}{2}\right)
\end{eqnarray*}
since $dy = \exp(x) dx$
and $\frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +\sigma_X^2))^2 }{2\sigma_X^2}\right)$
is the PDF of a random variable $\sim$ $\mathcal{N}(\mu_X+\sigma_X^2,\sigma_X^2)$,
thus
\begin{equation}
\label{eq:log-normal-mean}
\mu_Y
= \Expect Y = \exp\left(\frac{2\mu_X+ \sigma_X^2}{2}\right).
\end{equation}

Similarly,
\begin{eqnarray*}
\lefteqn{
\Expect Y^2 = \int_{0}^{\infty} y^2 f_Y(y) \, dy
= \int_{0}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  y \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right) dy
}
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp(x) \exp\left(-\frac{(x-\mu_X)^2}{2\sigma_X^2}\right) \exp(x) \, dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x-\mu_X)^2}{2\sigma_X^2}+2x\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{x^2 - 2(\mu_X +2\sigma_X^2)x + \mu_X^2}{2\sigma_X^2}\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +2\sigma_X^2))^2 + \mu_X^2 -(\mu_X+2\sigma_X^2)^2}{2\sigma_X^2}\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +2\sigma_X^2))^2 - 4\mu_X\sigma_X^2 - 4\sigma_X^4}{2\sigma_X^2}\right) dx
\\
&=&
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +2\sigma_X^2))^2 - 4\sigma_X^2(\mu_X+ \sigma_X^2)}{2\sigma_X^2}\right) dx
\\
&=&
\exp\left({2(\mu_X+ \sigma_X^2)}\right)
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma_X }  \exp\left(-\frac{(x - (\mu_X +2\sigma_X^2))^2 }{2\sigma_X^2}\right) dx
\\
&=&
\exp\left({2(\mu_X+ \sigma_X^2)}\right),
\end{eqnarray*}
thus
\begin{equation}
\label{eq:log-normal-var}
\sigma_Y^2 =
\Var(Y) = \Expect Y^2 - (\Expect Y)^2 = \exp(2(\mu_X+\sigma_X^2)) - \exp(2\mu_X+\sigma_X^2)
= (\exp(\sigma_X^2)-1) \exp(2\mu_X+\sigma_X^2)).
\end{equation}

Note that (\ref{eq:log-normal-mean}) implies that
\begin{equation}
\mu_Y^2
= \exp(2\mu_X+\sigma_X^2)),
\end{equation}
hence
\begin{equation}
\sigma_Y^2
= (\exp(\sigma_X^2)-1) \mu_Y^2.
\end{equation}
This multiplicative dependency of the standard deviation on the expected value
is attributed to the fact that $\log(Y) \sim \mathcal{N}(\mu_X,\sigma_X^2)$,
\ie,
the log-scale of $Y$ follows the normal distribution.

Now if we differentiate the PDF with respect to $y$,
(\ref{eq:log-normal-pdf}) implies that
\begin{eqnarray*}
\lefteqn{
\frac{d}{dy} f_Y(y)
= \frac{d}{dy} \left(\frac{1}{\sqrt{2\pi} \sigma_X y}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right)\right)
}
\\
&=&
-\frac{1}{\sqrt{2\pi} \sigma_X y^2}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right)
\\
&&
+
\frac{1}{\sqrt{2\pi} \sigma_X y}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right)
\left(-\frac{(\log(y)-\mu_X)}{\sigma_X^2}\right)
\frac{1}{y}
\\
&=&
-\frac{1}{\sqrt{2\pi} \sigma_X y^2}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right)
\left(1+\frac{(\log(y)-\mu_X)}{\sigma_X^2} \right)
\\
&=&
-\frac{1}{\sqrt{2\pi} \sigma_X^3 y^2}  \exp\left(-\frac{(\log(y)-\mu_X)^2}{2\sigma_X^2}\right)
\left(\log(y)-(\mu_X-\sigma_X^2) \right).
\end{eqnarray*}
Equating the derivative to zero yields
\begin{equation}
y = \exp(\mu_X-\sigma_X^2),
\end{equation}
which is the mode of $Y$.


\subsubsection{Parameter estimation}

Now assume that we have a log-normally distributed random variable, $Y\in\ppreals$,
with $\mu_Y$ and $\sigma_Y^2$ as its mean and variance.
We derived the parameters of the source distribution, $\mu_X$ and $\sigma_X$.

The two equations, (\ref{eq:log-normal-mean}) and (\ref{eq:log-normal-var}), imply
\begin{eqnarray*}
\mu_Y &=& \exp(\mu_X+\sigma_X^2/2),
\\
\sigma_Y^2 &=& (\exp(\sigma_X^2)-1)\exp(2\mu_X+\sigma_X^2) = (\exp(\sigma_X^2)-1) \mu_Y^2,
\end{eqnarray*}
thus
\begin{eqnarray*}
\sigma_X^2 &=& \log(1+{\sigma_Y^2}/{\mu_Y^2}),
\\
\mu_X &=& \log(\mu_Y) - \sigma_X^2/2 = \log(\mu_Y) - \log(1+{\sigma_Y^2}/{\mu_Y^2})/2
= \frac{1}{2} \log\left(\frac{\mu_Y^2}{1+{\sigma_Y^2}/{\mu_Y^2}}\right).
\end{eqnarray*}


\section{Exponential family}

In probability and statistics,
an exponential family is a parametric set of probability distributions of a certain form.
This special form is chosen for mathematical convenience, based on some useful algebraic properties,
as well as for generality,
as exponential families are in a sense very natural sets of distributions to consider.
The term exponential class is sometimes used in place of \emph{exponential family},
or the older term Koopman–Darmois family.
The terms \emph{distribution} and \emph{family} are often used loosely:
properly, an exponential family is a set of distributions,
where the specific distribution varies with the parameter;
however, a parametric family of distributions is often referred to as \emph{a distribution}
(like \emph{the normal distribution}, meaning \emph{the family of normal distributions})
and the set of all exponential families is sometimes loosely referred to as \emph{the} exponential family.

The concept of exponential families is credited to E. J. G. Pitman, G. Darmois, and B. O. Koopman in 1935–1936.
Exponential families of distributions provides a general framework for selecting a possible alternative parameterisation of a parametric family of distributions,
in terms of natural parameters, and for defining useful sample statistics, called the natural sufficient statistics of the family.

The exponential family of distributions over $x\in\reals^n$ given parameters $\eta \in\reals^m$ is defined to be the set of distributions of the form
\begin{equation}
\label{eq:pdf:exp-family}
p(x|\eta) = g(\eta) h(x) \exp(\eta^T u(x))
\end{equation}
for some functions $g:\reals^m \to \preals$, $h:\reals^n\to\preals$,
and $u:\reals^n \to \reals^m$.
Note that the requirement for the PDF implies
\begin{equation}
\label{eq:vbydiuv}
\int p(x|\eta) dx = g(\eta) \int h(x) \exp(\eta^T u(x)) dx = 1
\Leftrightarrow
g(\eta) = \left(\int h(x) \exp(\eta^T u(x)) dx \right)^{-1}.
\end{equation}
The exponential family of distributions covers many distributions as shown below.

\subsection{Categorical distribution}

For $\mu_i>0$ with $\sum_{i=1}^n \mu_i =1$,
the PMF of the categorical distribution
is given by
\begin{equation}
p(x|\mu) = \prod_{i=1}^n \mu_i^{x_i}
= \mu_n^{1-\sum_{j=1}^{n-1} x_j}\prod_{i=1}^{n-1} \mu_i^{x_i}
= \mu_n \prod_{i=1}^{n-1} (\mu_i / \mu_n)^{x_i}
\end{equation}
where $x \in \{0,1\}^{n-1}$ with $0\leq \sum_{i=1}^{n-1} x_i \leq 1$.
Since
\begin{equation}
p(x|\mu) = \mu_n \exp\left( \sum_{i=1}^{n-1} x_i \log(\mu_i / \mu_n) \right),
\end{equation}
we could let $\eta_i = \log(\mu_i / \mu_n)$ for $i=1, \ldots, n-1$
and figure out how we can express $\mu_n$ in terms of $\eta\in\reals^{n-1}$.
However, here we use (\ref{eq:vbydiuv}) for that.
Now let
\begin{equation}
p(x|\eta) = g(\eta) \exp(\eta^Tx).
\end{equation}
Then
\begin{equation}
g(\eta) \sum_{0\leq \ones^T x \leq 1} \exp(\eta^T x) = g(\eta) \left( 1 + \sum_{i=1}^{n-1} \exp(\eta_i)\right) = 1,
\end{equation}
thus
$g(\eta) = \left( 1 + \sum_{i=1}^{n-1} \exp(\eta_i)\right)^{-1}$.
Therefore the categorical distribution using the standard representation for the exponential family (\ref{eq:pdf:exp-family})
is given by
\begin{equation}
\label{eq:pdf:multinoulli:exp}
p(x|\eta) = \frac{\exp(\eta^T x)}{1 + \sum_{i=1}^{n-1} \exp(\eta_i)}
\end{equation}
with
\begin{eqnarray}
\eta_i &=& \log\left(\frac{\mu_i}{1-\ones^T \mu} \right)
\mbox{ for } i=1,\ldots,n-1
\\
u(x) &=& x
\\
g(\eta) &=& \left( 1 + \sum_{i=1}^{n-1} \exp(\eta_i)\right)^{-1}
\\
h(x) &=& 1.
\end{eqnarray}

\subsection{Bernoulli distribution}

The Bernoulli distribution is a categorical distribution with $n=2$.
Therefore (\ref{eq:pdf:multinoulli:exp}) implies that
the Bernoulli distribution using the standard representation for the exponential family (\ref{eq:pdf:exp-family})
is given by
\begin{equation}
\label{eq:pdf:bernoulli:exp}
p(x|\eta) = \frac{\exp(\eta x)}{1 + \exp(\eta)}
\end{equation}
where $x \in \{0,1\}$ and $\eta \in \reals$
with
\begin{eqnarray}
\eta &=& \log\left(\frac{\mu}{1-\mu} \right)
\\
u(x) &=& x
\\
g(\eta) &=& \left( 1 + \exp(\eta)\right)^{-1} = \sigma(-\eta)
\\
h(x) &=& 1
\end{eqnarray}
where $\sigma$ is the logistic sigmoid function.


\subsection{Gaussian distribution}

The PDF of Gaussian random variable with mean $\mu\in\reals^n$ and covariance matrix $\Sigma\in\posdefset{n}$ is
given by (\ref{eq:pdf:gaussian}). We can rewrite it as
\begin{equation}
p(x|\mu,\Sigma) = \frac{1}{(2\pi)^{n/2}\det(\Sigma)^{1/2}} \exp\left(
-\frac{1}{2} x^T \Sigma^{-1} x
+ \mu^T \Sigma^{-1} x - \frac{1}{2} \mu^T \Sigma^{-1} \mu
\right).
\end{equation}
If we let $\eta_1 = - \frac{1}{2} \Sigma^{-1}$ and $\eta_2 = \Sigma^{-1} \mu$,
then
\begin{eqnarray*}
p(x|\eta)
&=&
\frac{\det(-2\eta_1)^{1/2}}{(2\pi)^{n/2}} \exp\left(
\Tr \eta_1 xx^T + \eta_2^T x
- \frac{1}{2} \eta_2^T  (-2\eta_1)^{-1} \eta_2
\right)
\\
&=&
\frac{2^{n/2}\det(-\eta_1)^{1/2}}{(2\pi)^{n/2}} \exp\left(
\Tr \eta_1 xx^T + \eta_2^T x
+ \frac{1}{4} \eta_2^T  \eta_1^{-1} \eta_2
\right).
\end{eqnarray*}

Therefore
the Gaussian distribution using the standard representation for the exponential family (\ref{eq:pdf:exp-family})
is given by
\begin{equation}
\label{eq:pdf:gaussian:exp}
p(x|\eta) = \frac{\det(-\eta_1)^{1/2}}{\pi^{n/2}} \exp\left(\frac{1}{4} \eta_2^T  \eta_1^{-1} \eta_2 \right)
\exp\left( \Tr \eta_1 xx^T + \eta_2^T x \right)
\end{equation}
with
\begin{eqnarray}
\eta &=& (\eta_1, \eta_2) = \left(-\frac{1}{2} \Sigma^{-1}, \Sigma^{-1}\mu \right) \in (-\posdefset{n}) \times \reals^n
\\
u(x) &=& \left(xx^T, x\right) \in \possemidefset{n} \times \reals^n
\\
g(\eta) &=& \frac{\det(-\eta_1)^{1/2}}{\pi^{n/2}} \exp\left(\frac{1}{4} \eta_2^T  \eta_1^{-1} \eta_2 \right)
\\
h(x) &=& 1.
\end{eqnarray}



